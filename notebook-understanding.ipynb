{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the provided code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial imports and loading of utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "is_executing": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import dataloader\n",
    "from utils.utils import *\n",
    "\n",
    "#imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to plot the Tensor image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only for visualization purposes, so it's not necessary to understand in detail. If we end up using it, we will try to understand it fully. We currently don't understand where the unnormalization numbers come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the tensor image\n",
    "def show_image(img, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "\n",
    "    #unnormalize\n",
    "    img[0] = img[0] * 0.229\n",
    "    img[1] = img[1] * 0.224\n",
    "    img[2] = img[2] * 0.225\n",
    "    img[0] += 0.485\n",
    "    img[1] += 0.456\n",
    "    img[2] += 0.406\n",
    "\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "\n",
    "\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate the Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still don't understand where the normalization constants come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Iterations done out of 40455\n",
      "200 Iterations done out of 40455\n",
      "300 Iterations done out of 40455\n",
      "400 Iterations done out of 40455\n",
      "500 Iterations done out of 40455\n",
      "600 Iterations done out of 40455\n",
      "700 Iterations done out of 40455\n",
      "800 Iterations done out of 40455\n",
      "900 Iterations done out of 40455\n",
      "1000 Iterations done out of 40455\n",
      "1100 Iterations done out of 40455\n",
      "1200 Iterations done out of 40455\n",
      "1300 Iterations done out of 40455\n",
      "1400 Iterations done out of 40455\n",
      "1500 Iterations done out of 40455\n",
      "1600 Iterations done out of 40455\n",
      "1700 Iterations done out of 40455\n",
      "1800 Iterations done out of 40455\n",
      "1900 Iterations done out of 40455\n",
      "2000 Iterations done out of 40455\n",
      "2100 Iterations done out of 40455\n",
      "2200 Iterations done out of 40455\n",
      "2300 Iterations done out of 40455\n",
      "2400 Iterations done out of 40455\n",
      "2500 Iterations done out of 40455\n",
      "2600 Iterations done out of 40455\n",
      "2700 Iterations done out of 40455\n",
      "2800 Iterations done out of 40455\n",
      "2900 Iterations done out of 40455\n",
      "3000 Iterations done out of 40455\n",
      "3100 Iterations done out of 40455\n",
      "3200 Iterations done out of 40455\n",
      "3300 Iterations done out of 40455\n",
      "3400 Iterations done out of 40455\n",
      "3500 Iterations done out of 40455\n",
      "3600 Iterations done out of 40455\n",
      "3700 Iterations done out of 40455\n",
      "3800 Iterations done out of 40455\n",
      "3900 Iterations done out of 40455\n",
      "4000 Iterations done out of 40455\n",
      "4100 Iterations done out of 40455\n",
      "4200 Iterations done out of 40455\n",
      "4300 Iterations done out of 40455\n",
      "4400 Iterations done out of 40455\n",
      "4500 Iterations done out of 40455\n",
      "4600 Iterations done out of 40455\n",
      "4700 Iterations done out of 40455\n",
      "4800 Iterations done out of 40455\n",
      "4900 Iterations done out of 40455\n",
      "5000 Iterations done out of 40455\n",
      "5100 Iterations done out of 40455\n",
      "5200 Iterations done out of 40455\n",
      "5300 Iterations done out of 40455\n",
      "5400 Iterations done out of 40455\n",
      "5500 Iterations done out of 40455\n",
      "5600 Iterations done out of 40455\n",
      "5700 Iterations done out of 40455\n",
      "5800 Iterations done out of 40455\n",
      "5900 Iterations done out of 40455\n",
      "6000 Iterations done out of 40455\n",
      "6100 Iterations done out of 40455\n",
      "6200 Iterations done out of 40455\n",
      "6300 Iterations done out of 40455\n",
      "6400 Iterations done out of 40455\n",
      "6500 Iterations done out of 40455\n",
      "6600 Iterations done out of 40455\n",
      "6700 Iterations done out of 40455\n",
      "6800 Iterations done out of 40455\n",
      "6900 Iterations done out of 40455\n",
      "7000 Iterations done out of 40455\n",
      "7100 Iterations done out of 40455\n",
      "7200 Iterations done out of 40455\n",
      "7300 Iterations done out of 40455\n",
      "7400 Iterations done out of 40455\n",
      "7500 Iterations done out of 40455\n",
      "7600 Iterations done out of 40455\n",
      "7700 Iterations done out of 40455\n",
      "7800 Iterations done out of 40455\n",
      "7900 Iterations done out of 40455\n",
      "8000 Iterations done out of 40455\n",
      "8100 Iterations done out of 40455\n",
      "8200 Iterations done out of 40455\n",
      "8300 Iterations done out of 40455\n",
      "8400 Iterations done out of 40455\n",
      "8500 Iterations done out of 40455\n",
      "8600 Iterations done out of 40455\n",
      "8700 Iterations done out of 40455\n",
      "8800 Iterations done out of 40455\n",
      "8900 Iterations done out of 40455\n",
      "9000 Iterations done out of 40455\n",
      "9100 Iterations done out of 40455\n",
      "9200 Iterations done out of 40455\n",
      "9300 Iterations done out of 40455\n",
      "9400 Iterations done out of 40455\n",
      "9500 Iterations done out of 40455\n",
      "9600 Iterations done out of 40455\n",
      "9700 Iterations done out of 40455\n",
      "9800 Iterations done out of 40455\n",
      "9900 Iterations done out of 40455\n",
      "10000 Iterations done out of 40455\n",
      "10100 Iterations done out of 40455\n",
      "10200 Iterations done out of 40455\n",
      "10300 Iterations done out of 40455\n",
      "10400 Iterations done out of 40455\n",
      "10500 Iterations done out of 40455\n",
      "10600 Iterations done out of 40455\n",
      "10700 Iterations done out of 40455\n",
      "10800 Iterations done out of 40455\n",
      "10900 Iterations done out of 40455\n",
      "11000 Iterations done out of 40455\n",
      "11100 Iterations done out of 40455\n",
      "11200 Iterations done out of 40455\n",
      "11300 Iterations done out of 40455\n",
      "11400 Iterations done out of 40455\n",
      "11500 Iterations done out of 40455\n",
      "11600 Iterations done out of 40455\n",
      "11700 Iterations done out of 40455\n",
      "11800 Iterations done out of 40455\n",
      "11900 Iterations done out of 40455\n",
      "12000 Iterations done out of 40455\n",
      "12100 Iterations done out of 40455\n",
      "12200 Iterations done out of 40455\n",
      "12300 Iterations done out of 40455\n",
      "12400 Iterations done out of 40455\n",
      "12500 Iterations done out of 40455\n",
      "12600 Iterations done out of 40455\n",
      "12700 Iterations done out of 40455\n",
      "12800 Iterations done out of 40455\n",
      "12900 Iterations done out of 40455\n",
      "13000 Iterations done out of 40455\n",
      "13100 Iterations done out of 40455\n",
      "13200 Iterations done out of 40455\n",
      "13300 Iterations done out of 40455\n",
      "13400 Iterations done out of 40455\n",
      "13500 Iterations done out of 40455\n",
      "13600 Iterations done out of 40455\n",
      "13700 Iterations done out of 40455\n",
      "13800 Iterations done out of 40455\n",
      "13900 Iterations done out of 40455\n",
      "14000 Iterations done out of 40455\n",
      "14100 Iterations done out of 40455\n",
      "14200 Iterations done out of 40455\n",
      "14300 Iterations done out of 40455\n",
      "14400 Iterations done out of 40455\n",
      "14500 Iterations done out of 40455\n",
      "14600 Iterations done out of 40455\n",
      "14700 Iterations done out of 40455\n",
      "14800 Iterations done out of 40455\n",
      "14900 Iterations done out of 40455\n",
      "15000 Iterations done out of 40455\n",
      "15100 Iterations done out of 40455\n",
      "15200 Iterations done out of 40455\n",
      "15300 Iterations done out of 40455\n",
      "15400 Iterations done out of 40455\n",
      "15500 Iterations done out of 40455\n",
      "15600 Iterations done out of 40455\n",
      "15700 Iterations done out of 40455\n",
      "15800 Iterations done out of 40455\n",
      "15900 Iterations done out of 40455\n",
      "16000 Iterations done out of 40455\n",
      "16100 Iterations done out of 40455\n",
      "16200 Iterations done out of 40455\n",
      "16300 Iterations done out of 40455\n",
      "16400 Iterations done out of 40455\n",
      "16500 Iterations done out of 40455\n",
      "16600 Iterations done out of 40455\n",
      "16700 Iterations done out of 40455\n",
      "16800 Iterations done out of 40455\n",
      "16900 Iterations done out of 40455\n",
      "17000 Iterations done out of 40455\n",
      "17100 Iterations done out of 40455\n",
      "17200 Iterations done out of 40455\n",
      "17300 Iterations done out of 40455\n",
      "17400 Iterations done out of 40455\n",
      "17500 Iterations done out of 40455\n",
      "17600 Iterations done out of 40455\n",
      "17700 Iterations done out of 40455\n",
      "17800 Iterations done out of 40455\n",
      "17900 Iterations done out of 40455\n",
      "18000 Iterations done out of 40455\n",
      "18100 Iterations done out of 40455\n",
      "18200 Iterations done out of 40455\n",
      "18300 Iterations done out of 40455\n",
      "18400 Iterations done out of 40455\n",
      "18500 Iterations done out of 40455\n",
      "18600 Iterations done out of 40455\n",
      "18700 Iterations done out of 40455\n",
      "18800 Iterations done out of 40455\n",
      "18900 Iterations done out of 40455\n",
      "19000 Iterations done out of 40455\n",
      "19100 Iterations done out of 40455\n",
      "19200 Iterations done out of 40455\n",
      "19300 Iterations done out of 40455\n",
      "19400 Iterations done out of 40455\n",
      "19500 Iterations done out of 40455\n",
      "19600 Iterations done out of 40455\n",
      "19700 Iterations done out of 40455\n",
      "19800 Iterations done out of 40455\n",
      "19900 Iterations done out of 40455\n",
      "20000 Iterations done out of 40455\n",
      "20100 Iterations done out of 40455\n",
      "20200 Iterations done out of 40455\n",
      "20300 Iterations done out of 40455\n",
      "20400 Iterations done out of 40455\n",
      "20500 Iterations done out of 40455\n",
      "20600 Iterations done out of 40455\n",
      "20700 Iterations done out of 40455\n",
      "20800 Iterations done out of 40455\n",
      "20900 Iterations done out of 40455\n",
      "21000 Iterations done out of 40455\n",
      "21100 Iterations done out of 40455\n",
      "21200 Iterations done out of 40455\n",
      "21300 Iterations done out of 40455\n",
      "21400 Iterations done out of 40455\n",
      "21500 Iterations done out of 40455\n",
      "21600 Iterations done out of 40455\n",
      "21700 Iterations done out of 40455\n",
      "21800 Iterations done out of 40455\n",
      "21900 Iterations done out of 40455\n",
      "22000 Iterations done out of 40455\n",
      "22100 Iterations done out of 40455\n",
      "22200 Iterations done out of 40455\n",
      "22300 Iterations done out of 40455\n",
      "22400 Iterations done out of 40455\n",
      "22500 Iterations done out of 40455\n",
      "22600 Iterations done out of 40455\n",
      "22700 Iterations done out of 40455\n",
      "22800 Iterations done out of 40455\n",
      "22900 Iterations done out of 40455\n",
      "23000 Iterations done out of 40455\n",
      "23100 Iterations done out of 40455\n",
      "23200 Iterations done out of 40455\n",
      "23300 Iterations done out of 40455\n",
      "23400 Iterations done out of 40455\n",
      "23500 Iterations done out of 40455\n",
      "23600 Iterations done out of 40455\n",
      "23700 Iterations done out of 40455\n",
      "23800 Iterations done out of 40455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23900 Iterations done out of 40455\n",
      "24000 Iterations done out of 40455\n",
      "24100 Iterations done out of 40455\n",
      "24200 Iterations done out of 40455\n",
      "24300 Iterations done out of 40455\n",
      "24400 Iterations done out of 40455\n",
      "24500 Iterations done out of 40455\n",
      "24600 Iterations done out of 40455\n",
      "24700 Iterations done out of 40455\n",
      "24800 Iterations done out of 40455\n",
      "24900 Iterations done out of 40455\n",
      "25000 Iterations done out of 40455\n",
      "25100 Iterations done out of 40455\n",
      "25200 Iterations done out of 40455\n",
      "25300 Iterations done out of 40455\n",
      "25400 Iterations done out of 40455\n",
      "25500 Iterations done out of 40455\n",
      "25600 Iterations done out of 40455\n",
      "25700 Iterations done out of 40455\n",
      "25800 Iterations done out of 40455\n",
      "25900 Iterations done out of 40455\n",
      "26000 Iterations done out of 40455\n",
      "26100 Iterations done out of 40455\n",
      "26200 Iterations done out of 40455\n",
      "26300 Iterations done out of 40455\n",
      "26400 Iterations done out of 40455\n",
      "26500 Iterations done out of 40455\n",
      "26600 Iterations done out of 40455\n",
      "26700 Iterations done out of 40455\n",
      "26800 Iterations done out of 40455\n",
      "26900 Iterations done out of 40455\n",
      "27000 Iterations done out of 40455\n",
      "27100 Iterations done out of 40455\n",
      "27200 Iterations done out of 40455\n",
      "27300 Iterations done out of 40455\n",
      "27400 Iterations done out of 40455\n",
      "27500 Iterations done out of 40455\n",
      "27600 Iterations done out of 40455\n",
      "27700 Iterations done out of 40455\n",
      "27800 Iterations done out of 40455\n",
      "27900 Iterations done out of 40455\n",
      "28000 Iterations done out of 40455\n",
      "28100 Iterations done out of 40455\n",
      "28200 Iterations done out of 40455\n",
      "28300 Iterations done out of 40455\n",
      "28400 Iterations done out of 40455\n",
      "28500 Iterations done out of 40455\n",
      "28600 Iterations done out of 40455\n",
      "28700 Iterations done out of 40455\n",
      "28800 Iterations done out of 40455\n",
      "28900 Iterations done out of 40455\n",
      "29000 Iterations done out of 40455\n",
      "29100 Iterations done out of 40455\n",
      "29200 Iterations done out of 40455\n",
      "29300 Iterations done out of 40455\n",
      "29400 Iterations done out of 40455\n",
      "29500 Iterations done out of 40455\n",
      "29600 Iterations done out of 40455\n",
      "29700 Iterations done out of 40455\n",
      "29800 Iterations done out of 40455\n",
      "29900 Iterations done out of 40455\n",
      "30000 Iterations done out of 40455\n",
      "30100 Iterations done out of 40455\n",
      "30200 Iterations done out of 40455\n",
      "30300 Iterations done out of 40455\n",
      "30400 Iterations done out of 40455\n",
      "30500 Iterations done out of 40455\n",
      "30600 Iterations done out of 40455\n",
      "30700 Iterations done out of 40455\n",
      "30800 Iterations done out of 40455\n",
      "30900 Iterations done out of 40455\n",
      "31000 Iterations done out of 40455\n",
      "31100 Iterations done out of 40455\n",
      "31200 Iterations done out of 40455\n",
      "31300 Iterations done out of 40455\n",
      "31400 Iterations done out of 40455\n",
      "31500 Iterations done out of 40455\n",
      "31600 Iterations done out of 40455\n",
      "31700 Iterations done out of 40455\n",
      "31800 Iterations done out of 40455\n",
      "31900 Iterations done out of 40455\n",
      "32000 Iterations done out of 40455\n",
      "32100 Iterations done out of 40455\n",
      "32200 Iterations done out of 40455\n",
      "32300 Iterations done out of 40455\n",
      "32400 Iterations done out of 40455\n",
      "32500 Iterations done out of 40455\n",
      "32600 Iterations done out of 40455\n",
      "32700 Iterations done out of 40455\n",
      "32800 Iterations done out of 40455\n",
      "32900 Iterations done out of 40455\n",
      "33000 Iterations done out of 40455\n",
      "33100 Iterations done out of 40455\n",
      "33200 Iterations done out of 40455\n",
      "33300 Iterations done out of 40455\n",
      "33400 Iterations done out of 40455\n",
      "33500 Iterations done out of 40455\n",
      "33600 Iterations done out of 40455\n",
      "33700 Iterations done out of 40455\n",
      "33800 Iterations done out of 40455\n",
      "33900 Iterations done out of 40455\n",
      "34000 Iterations done out of 40455\n",
      "34100 Iterations done out of 40455\n",
      "34200 Iterations done out of 40455\n",
      "34300 Iterations done out of 40455\n",
      "34400 Iterations done out of 40455\n",
      "34500 Iterations done out of 40455\n",
      "34600 Iterations done out of 40455\n",
      "34700 Iterations done out of 40455\n",
      "34800 Iterations done out of 40455\n",
      "34900 Iterations done out of 40455\n",
      "35000 Iterations done out of 40455\n",
      "35100 Iterations done out of 40455\n",
      "35200 Iterations done out of 40455\n",
      "35300 Iterations done out of 40455\n",
      "35400 Iterations done out of 40455\n",
      "35500 Iterations done out of 40455\n",
      "35600 Iterations done out of 40455\n",
      "35700 Iterations done out of 40455\n",
      "35800 Iterations done out of 40455\n",
      "35900 Iterations done out of 40455\n",
      "36000 Iterations done out of 40455\n",
      "36100 Iterations done out of 40455\n",
      "36200 Iterations done out of 40455\n",
      "36300 Iterations done out of 40455\n",
      "36400 Iterations done out of 40455\n",
      "36500 Iterations done out of 40455\n",
      "36600 Iterations done out of 40455\n",
      "36700 Iterations done out of 40455\n",
      "36800 Iterations done out of 40455\n",
      "36900 Iterations done out of 40455\n",
      "37000 Iterations done out of 40455\n",
      "37100 Iterations done out of 40455\n",
      "37200 Iterations done out of 40455\n",
      "37300 Iterations done out of 40455\n",
      "37400 Iterations done out of 40455\n",
      "37500 Iterations done out of 40455\n",
      "37600 Iterations done out of 40455\n",
      "37700 Iterations done out of 40455\n",
      "37800 Iterations done out of 40455\n",
      "37900 Iterations done out of 40455\n",
      "38000 Iterations done out of 40455\n",
      "38100 Iterations done out of 40455\n",
      "38200 Iterations done out of 40455\n",
      "38300 Iterations done out of 40455\n",
      "38400 Iterations done out of 40455\n",
      "38500 Iterations done out of 40455\n",
      "38600 Iterations done out of 40455\n",
      "38700 Iterations done out of 40455\n",
      "38800 Iterations done out of 40455\n",
      "38900 Iterations done out of 40455\n",
      "39000 Iterations done out of 40455\n",
      "39100 Iterations done out of 40455\n",
      "39200 Iterations done out of 40455\n",
      "39300 Iterations done out of 40455\n",
      "39400 Iterations done out of 40455\n",
      "39500 Iterations done out of 40455\n",
      "39600 Iterations done out of 40455\n",
      "39700 Iterations done out of 40455\n",
      "39800 Iterations done out of 40455\n",
      "39900 Iterations done out of 40455\n",
      "40000 Iterations done out of 40455\n",
      "40100 Iterations done out of 40455\n",
      "40200 Iterations done out of 40455\n",
      "40300 Iterations done out of 40455\n",
      "40400 Iterations done out of 40455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting the constants\n",
    "data_location =  \"./data\"\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKER = 4\n",
    "\n",
    "#defining the transform to be applied\n",
    "transforms = T.Compose([\n",
    "    T.Resize(226),\n",
    "    T.RandomCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "#initialize the dataset class\n",
    "dataset =  FlickrDataset(\n",
    "    root_dir = data_location+\"/Images\",\n",
    "    captions_file = data_location+\"/captions.txt\",\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "joblib.dump(dataset, \"./processed_dataset.joblib\")\n",
    "\n",
    "\n",
    "\n",
    "#writing the dataloader\n",
    "data_loader = make_loader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=True,\n",
    "    # batch_first=False\n",
    ")\n",
    "\n",
    "#vocab_size\n",
    "vocab_size = len(dataset.vocab)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses attention in an encoder decoder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n",
    "        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n",
    "        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bahdanau Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim,attention_dim)\n",
    "        \n",
    "        self.A = nn.Linear(attention_dim,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, features, hidden_state):\n",
    "        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n",
    "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
    "        \n",
    "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n",
    "        \n",
    "        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n",
    "        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n",
    "        \n",
    "        \n",
    "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n",
    "        \n",
    "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n",
    "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n",
    "        \n",
    "        return alpha,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention Decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #save the model param\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n",
    "        \n",
    "        \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        \n",
    "        \n",
    "        self.fcn = nn.Linear(decoder_dim,vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        #vectorize the caption\n",
    "        embeds = self.embedding(captions)\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        #get the seq length to iterate\n",
    "        seq_length = len(captions[0])-1 #Exclude the last one\n",
    "        batch_size = captions.size(0)\n",
    "        num_features = features.size(1)\n",
    "        \n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
    "                \n",
    "        for s in range(seq_length):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                    \n",
    "            output = self.fcn(self.drop(h))\n",
    "            \n",
    "            preds[:,s] = output\n",
    "            alphas[:,s] = alpha  \n",
    "        \n",
    "        \n",
    "        return preds, alphas\n",
    "    \n",
    "    def generate_caption(self,features,max_len=20,vocab=None):\n",
    "        # Inference part\n",
    "        # Given the image features generate the captions\n",
    "        \n",
    "        batch_size = features.size(0)\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        alphas = []\n",
    "        \n",
    "        #starting input\n",
    "        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
    "        embeds = self.embedding(word)\n",
    "\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            \n",
    "            \n",
    "            #store the apla score\n",
    "            alphas.append(alpha.cpu().detach().numpy())\n",
    "            \n",
    "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(self.drop(h))\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            \n",
    "            #select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            #save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            #end if <EOS detected>\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            #send generated word as the next caption\n",
    "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [vocab.itos[idx] for idx in captions],alphas\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderRNN(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size = len(dataset.vocab),\n",
    "            attention_dim=attention_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters and Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "embed_size=300\n",
    "vocab_size = len(dataset.vocab)\n",
    "attention_dim=256\n",
    "encoder_dim=2048\n",
    "decoder_dim=512\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init model\n",
    "model = EncoderDecoder(\n",
    "    embed_size=300,\n",
    "    vocab_size = len(dataset.vocab),\n",
    "    attention_dim=256,\n",
    "    encoder_dim=2048,\n",
    "    decoder_dim=512\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to save the model\n",
    "def save_model(model,num_epochs):\n",
    "    model_state = {\n",
    "        'num_epochs':num_epochs,\n",
    "        'embed_size':embed_size,\n",
    "        'vocab_size':len(dataset.vocab),\n",
    "        'attention_dim':attention_dim,\n",
    "        'encoder_dim':encoder_dim,\n",
    "        'decoder_dim':decoder_dim,\n",
    "        'state_dict':model.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(model_state,'attention_model_state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(1,num_epochs+1):   \n",
    "    for idx, (image, captions) in enumerate(iter(data_loader)):\n",
    "        image,captions = image.to(device),captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        outputs,attentions = model(image, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        targets = captions[:,1:]\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx+1)%print_every == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "            \n",
    "            #generate the caption\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dataiter = iter(data_loader)\n",
    "                img,_ = next(dataiter)\n",
    "                features = model.encoder(img[0:1].to(device))\n",
    "                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
    "                caption = ' '.join(caps)\n",
    "                show_image(img[0],title=caption)\n",
    "                \n",
    "            model.train()\n",
    "        \n",
    "    #save the latest model\n",
    "    save_model(model,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate caption\n",
    "def get_caps_from(features_tensors):\n",
    "    #generate the caption\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(features_tensors.to(device))\n",
    "        caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
    "        caption = ' '.join(caps)\n",
    "        show_image(features_tensors[0],title=caption)\n",
    "    \n",
    "    return caps,alphas\n",
    "\n",
    "#Show attention\n",
    "def plot_attention(img, result, attention_plot):\n",
    "    #untransform\n",
    "    img[0] = img[0] * 0.229\n",
    "    img[1] = img[1] * 0.224 \n",
    "    img[2] = img[2] * 0.225 \n",
    "    img[0] += 0.485 \n",
    "    img[1] += 0.456 \n",
    "    img[2] += 0.406\n",
    "    \n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    temp_image = img\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = attention_plot[l].reshape(7,7)\n",
    "        \n",
    "        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show any 1\n",
    "dataiter = iter(data_loader)\n",
    "images,_ = next(dataiter)\n",
    "\n",
    "img = images[0].detach().clone()\n",
    "img1 = images[0].detach().clone()\n",
    "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
    "\n",
    "plot_attention(img1, caps, alphas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
