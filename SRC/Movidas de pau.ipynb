{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a69d37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Pau\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\wandb\\run-20230530_130424-abo2hbdy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grup10/pytorch-demo/runs/abo2hbdy' target=\"_blank\">devout-salad-227</a></strong> to <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grup10/pytorch-demo/runs/abo2hbdy' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/abo2hbdy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing_time: 53.449002265930176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13df95162d2340cfbcb6e04f8346c45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00050 examples: 8.017\n",
      "Loss after 00100 examples: 7.989\n",
      "Loss after 00150 examples: 7.955\n",
      "Loss after 00200 examples: 7.931\n",
      "Loss after 00250 examples: 7.891\n",
      "Loss after 00300 examples: 7.845\n",
      "Loss after 00350 examples: 7.818\n",
      "Loss after 00400 examples: 7.755\n",
      "Loss after 00450 examples: 7.768\n",
      "Loss after 00500 examples: 7.701\n",
      "Loss after 00550 examples: 7.619\n",
      "Loss after 00600 examples: 7.558\n",
      "Loss after 00650 examples: 7.512\n",
      "Loss after 00700 examples: 7.488\n",
      "Loss after 00750 examples: 7.353\n",
      "Loss after 00800 examples: 7.316\n",
      "Loss after 00850 examples: 7.200\n",
      "Loss after 00900 examples: 7.179\n",
      "Loss after 00950 examples: 7.141\n",
      "Loss after 01000 examples: 7.010\n",
      "Loss after 01050 examples: 6.873\n",
      "Loss after 01100 examples: 6.796\n",
      "Loss after 01150 examples: 6.746\n",
      "Loss after 01200 examples: 6.628\n",
      "Loss after 01250 examples: 6.497\n",
      "Loss after 01300 examples: 6.479\n",
      "Loss after 01350 examples: 6.426\n",
      "Loss after 01400 examples: 6.260\n",
      "Loss after 01450 examples: 6.288\n",
      "Loss after 01500 examples: 6.027\n",
      "Loss after 01550 examples: 6.079\n",
      "Loss after 01600 examples: 5.988\n",
      "Loss after 01650 examples: 5.864\n",
      "Loss after 01700 examples: 5.922\n",
      "Loss after 01750 examples: 5.532\n",
      "Loss after 01800 examples: 5.579\n",
      "Loss after 01850 examples: 5.601\n",
      "Loss after 01900 examples: 5.448\n",
      "Loss after 01950 examples: 5.564\n",
      "Loss after 02000 examples: 5.671\n",
      "Loss after 02050 examples: 5.457\n",
      "Loss after 02100 examples: 5.414\n",
      "Loss after 02150 examples: 5.383\n",
      "Loss after 02200 examples: 5.279\n",
      "Loss after 02250 examples: 5.370\n",
      "Loss after 02300 examples: 5.458\n",
      "Loss after 02350 examples: 5.212\n",
      "Loss after 02400 examples: 5.228\n",
      "Loss after 02450 examples: 5.227\n",
      "Loss after 02500 examples: 5.149\n",
      "Loss after 02550 examples: 5.162\n",
      "Loss after 02600 examples: 5.125\n",
      "Loss after 02650 examples: 5.170\n",
      "Loss after 02700 examples: 5.109\n",
      "Loss after 02750 examples: 5.032\n",
      "Loss after 02800 examples: 5.001\n",
      "Loss after 02850 examples: 5.186\n",
      "Loss after 02900 examples: 4.890\n",
      "Loss after 02950 examples: 4.911\n",
      "Loss after 03000 examples: 5.098\n",
      "Loss after 03050 examples: 5.105\n",
      "Loss after 03100 examples: 4.830\n",
      "Loss after 03150 examples: 5.051\n",
      "Loss after 03200 examples: 5.068\n",
      "Loss after 03250 examples: 4.974\n",
      "Loss after 03300 examples: 4.998\n",
      "Loss after 03350 examples: 5.047\n",
      "Loss after 03400 examples: 5.047\n",
      "Loss after 03450 examples: 4.991\n",
      "Loss after 03500 examples: 5.084\n",
      "Loss after 03550 examples: 4.848\n",
      "Loss after 03600 examples: 4.981\n",
      "Loss after 03650 examples: 4.789\n",
      "Loss after 03700 examples: 4.952\n",
      "Loss after 03750 examples: 4.739\n",
      "Loss after 03800 examples: 4.810\n",
      "Loss after 03850 examples: 4.947\n",
      "Loss after 03900 examples: 4.796\n",
      "Loss after 03950 examples: 4.895\n",
      "Loss after 04000 examples: 4.882\n",
      "Loss after 04050 examples: 4.790\n",
      "Loss after 04100 examples: 4.991\n",
      "Loss after 04150 examples: 4.848\n",
      "Loss after 04200 examples: 4.846\n",
      "Loss after 04250 examples: 4.806\n",
      "Loss after 04300 examples: 4.742\n",
      "Loss after 04350 examples: 4.785\n",
      "Loss after 04400 examples: 4.855\n",
      "Loss after 04450 examples: 4.876\n",
      "Loss after 04500 examples: 4.712\n",
      "Loss after 04550 examples: 4.858\n",
      "Loss after 04600 examples: 4.750\n",
      "Loss after 04650 examples: 4.809\n",
      "Loss after 04700 examples: 4.720\n",
      "Loss after 04750 examples: 4.781\n",
      "Loss after 04800 examples: 4.857\n",
      "Loss after 04850 examples: 4.806\n",
      "Loss after 04900 examples: 4.730\n",
      "Loss after 04950 examples: 4.668\n",
      "Loss after 05000 examples: 4.965\n",
      "Loss after 05050 examples: 4.836\n",
      "Loss after 05100 examples: 4.649\n",
      "Loss after 05150 examples: 4.902\n",
      "Loss after 05200 examples: 4.727\n",
      "Loss after 05250 examples: 4.773\n",
      "Loss after 05300 examples: 4.714\n",
      "Loss after 05350 examples: 4.599\n",
      "Loss after 05400 examples: 4.797\n",
      "Loss after 05450 examples: 4.666\n",
      "Loss after 05500 examples: 4.585\n",
      "Loss after 05550 examples: 4.607\n",
      "Loss after 05600 examples: 4.724\n",
      "Loss after 05650 examples: 4.863\n",
      "Loss after 05700 examples: 4.743\n",
      "Loss after 05750 examples: 4.667\n",
      "Loss after 05800 examples: 4.731\n",
      "Loss after 05850 examples: 4.701\n",
      "Loss after 05900 examples: 4.749\n",
      "Loss after 05950 examples: 4.621\n",
      "Loss after 06000 examples: 4.677\n",
      "Loss after 06050 examples: 4.745\n",
      "Loss after 06100 examples: 4.665\n",
      "Loss after 06150 examples: 4.604\n",
      "Loss after 06200 examples: 4.598\n",
      "Loss after 06250 examples: 4.743\n",
      "Loss after 06300 examples: 4.582\n",
      "Loss after 06350 examples: 4.739\n",
      "Loss after 06400 examples: 4.663\n",
      "Loss after 06450 examples: 4.733\n",
      "Loss after 06500 examples: 4.641\n",
      "Loss after 06550 examples: 4.677\n",
      "Loss after 06600 examples: 4.595\n",
      "Loss after 06650 examples: 4.578\n",
      "Loss after 06700 examples: 4.636\n",
      "Loss after 06750 examples: 4.807\n",
      "Loss after 06800 examples: 4.591\n",
      "Loss after 06850 examples: 4.796\n",
      "Loss after 06900 examples: 4.429\n",
      "Loss after 06950 examples: 4.647\n",
      "Loss after 07000 examples: 4.580\n",
      "Loss after 07050 examples: 4.557\n",
      "Loss after 07100 examples: 4.619\n",
      "Loss after 07150 examples: 4.623\n",
      "Loss after 07200 examples: 4.632\n",
      "Loss after 07250 examples: 4.608\n",
      "Loss after 07300 examples: 4.727\n",
      "Loss after 07350 examples: 4.625\n",
      "Loss after 07400 examples: 4.682\n",
      "Loss after 07450 examples: 4.404\n",
      "Loss after 07500 examples: 4.656\n",
      "Loss after 07550 examples: 4.523\n",
      "Loss after 07600 examples: 4.428\n",
      "Loss after 07650 examples: 4.556\n",
      "Loss after 07700 examples: 4.506\n",
      "Loss after 07750 examples: 4.544\n",
      "Loss after 07800 examples: 4.508\n",
      "Loss after 07850 examples: 4.469\n",
      "Loss after 07900 examples: 4.510\n",
      "Loss after 07950 examples: 4.634\n",
      "Loss after 08000 examples: 4.682\n",
      "Loss after 08050 examples: 4.499\n",
      "Loss after 08100 examples: 4.675\n",
      "Loss after 08150 examples: 4.636\n",
      "Loss after 08200 examples: 4.628\n",
      "Loss after 08250 examples: 4.297\n",
      "Loss after 08300 examples: 4.483\n",
      "Loss after 08350 examples: 4.482\n",
      "Loss after 08400 examples: 4.397\n",
      "Loss after 08450 examples: 4.535\n",
      "Loss after 08500 examples: 4.529\n",
      "Loss after 08550 examples: 4.494\n",
      "Loss after 08600 examples: 4.549\n",
      "Loss after 08650 examples: 4.458\n",
      "Loss after 08700 examples: 4.580\n",
      "Loss after 08750 examples: 4.463\n",
      "Loss after 08800 examples: 4.457\n",
      "Loss after 08850 examples: 4.455\n",
      "Loss after 08900 examples: 4.680\n",
      "Loss after 08950 examples: 4.546\n",
      "Loss after 09000 examples: 4.530\n",
      "Loss after 09050 examples: 4.468\n",
      "Loss after 09100 examples: 4.570\n",
      "Loss after 09150 examples: 4.421\n",
      "Loss after 09200 examples: 4.651\n",
      "Loss after 09250 examples: 4.394\n",
      "Loss after 09300 examples: 4.455\n",
      "Loss after 09350 examples: 4.559\n",
      "Loss after 09400 examples: 4.451\n",
      "Loss after 09450 examples: 4.441\n",
      "Loss after 09500 examples: 4.598\n",
      "Loss after 09550 examples: 4.457\n",
      "Loss after 09600 examples: 4.588\n",
      "Loss after 09650 examples: 4.392\n",
      "Loss after 09700 examples: 4.465\n",
      "Loss after 09750 examples: 4.417\n",
      "Loss after 09800 examples: 4.345\n",
      "Loss after 09850 examples: 4.363\n",
      "Loss after 09900 examples: 4.455\n",
      "Loss after 09950 examples: 4.425\n",
      "Loss after 10000 examples: 4.376\n",
      "Loss after 10050 examples: 4.338\n",
      "Loss after 10100 examples: 4.376\n",
      "Loss after 10150 examples: 4.442\n",
      "Loss after 10200 examples: 4.643\n",
      "Loss after 10250 examples: 4.524\n",
      "Loss after 10300 examples: 4.336\n",
      "Loss after 10350 examples: 4.337\n",
      "Loss after 10400 examples: 4.481\n",
      "Loss after 10450 examples: 4.314\n",
      "Loss after 10500 examples: 4.514\n",
      "Loss after 10550 examples: 4.455\n",
      "Loss after 10600 examples: 4.412\n",
      "Loss after 10650 examples: 4.460\n",
      "Loss after 10700 examples: 4.312\n",
      "Loss after 10750 examples: 4.224\n",
      "Loss after 10800 examples: 4.362\n",
      "Loss after 10850 examples: 4.461\n",
      "Loss after 10900 examples: 4.555\n",
      "Loss after 10950 examples: 4.412\n",
      "Loss after 11000 examples: 4.271\n",
      "Loss after 11050 examples: 4.376\n",
      "Loss after 11100 examples: 4.236\n",
      "Loss after 11150 examples: 4.608\n",
      "Loss after 11200 examples: 4.330\n",
      "Loss after 11250 examples: 4.365\n",
      "Loss after 11300 examples: 4.272\n",
      "Loss after 11350 examples: 4.557\n",
      "Loss after 11400 examples: 4.380\n",
      "Loss after 11450 examples: 4.313\n",
      "Loss after 11500 examples: 4.362\n",
      "Loss after 11550 examples: 4.387\n",
      "Loss after 11600 examples: 4.387\n",
      "Loss after 11650 examples: 4.274\n",
      "Loss after 11700 examples: 4.268\n",
      "Loss after 11750 examples: 4.490\n",
      "Loss after 11800 examples: 4.466\n",
      "Loss after 11850 examples: 4.262\n",
      "Loss after 11900 examples: 4.497\n",
      "Loss after 11950 examples: 4.305\n",
      "Loss after 12000 examples: 4.601\n",
      "Loss after 12050 examples: 4.466\n",
      "Loss after 12100 examples: 4.359\n",
      "Loss after 12150 examples: 4.295\n",
      "Loss after 12200 examples: 4.461\n",
      "Loss after 12250 examples: 4.228\n",
      "Loss after 12300 examples: 4.568\n",
      "Loss after 12350 examples: 4.347\n",
      "Loss after 12400 examples: 4.443\n",
      "Loss after 12450 examples: 4.365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 12500 examples: 4.371\n",
      "Loss after 12550 examples: 4.218\n",
      "Loss after 12600 examples: 4.453\n",
      "Loss after 12650 examples: 4.376\n",
      "Loss after 12700 examples: 4.176\n",
      "Loss after 12750 examples: 4.474\n",
      "Loss after 12800 examples: 4.209\n",
      "Loss after 12850 examples: 4.284\n",
      "Loss after 12900 examples: 4.360\n",
      "Loss after 12950 examples: 4.360\n",
      "Loss after 13000 examples: 4.321\n",
      "Loss after 13050 examples: 4.187\n",
      "Loss after 13100 examples: 4.304\n",
      "Loss after 13150 examples: 4.323\n",
      "Loss after 13200 examples: 4.343\n",
      "Loss after 13250 examples: 4.431\n",
      "Loss after 13300 examples: 4.323\n",
      "Loss after 13350 examples: 4.266\n",
      "Loss after 13400 examples: 4.205\n",
      "Loss after 13450 examples: 4.297\n",
      "Loss after 13500 examples: 4.322\n",
      "Loss after 13550 examples: 4.340\n",
      "Loss after 13600 examples: 4.337\n",
      "Loss after 13650 examples: 4.217\n",
      "Loss after 13700 examples: 4.204\n",
      "Loss after 13750 examples: 4.112\n",
      "Loss after 13800 examples: 4.356\n",
      "Loss after 13850 examples: 4.436\n",
      "Loss after 13900 examples: 4.242\n",
      "Loss after 13950 examples: 4.316\n",
      "Loss after 14000 examples: 4.217\n",
      "Loss after 14050 examples: 4.326\n",
      "Loss after 14100 examples: 4.305\n",
      "Loss after 14150 examples: 4.256\n",
      "Loss after 14200 examples: 4.230\n",
      "Loss after 14250 examples: 4.305\n",
      "Loss after 14300 examples: 4.300\n",
      "Loss after 14350 examples: 4.157\n",
      "Loss after 14400 examples: 4.136\n",
      "Loss after 14450 examples: 4.201\n",
      "Loss after 14500 examples: 4.344\n",
      "Loss after 14550 examples: 4.273\n",
      "Loss after 14600 examples: 4.259\n",
      "Loss after 14650 examples: 4.137\n",
      "Loss after 14700 examples: 4.193\n",
      "Loss after 14750 examples: 4.061\n",
      "Loss after 14800 examples: 4.197\n",
      "Loss after 14850 examples: 4.327\n",
      "Loss after 14900 examples: 4.362\n",
      "Loss after 14950 examples: 4.225\n",
      "Loss after 15000 examples: 4.250\n",
      "Loss after 15050 examples: 4.165\n",
      "Loss after 15100 examples: 4.298\n",
      "Loss after 15150 examples: 4.162\n",
      "Loss after 15200 examples: 4.235\n",
      "Loss after 15250 examples: 4.293\n",
      "Loss after 15300 examples: 4.320\n",
      "Loss after 15350 examples: 4.279\n",
      "Loss after 15400 examples: 4.036\n",
      "Loss after 15450 examples: 4.195\n",
      "Loss after 15500 examples: 4.207\n",
      "Loss after 15550 examples: 4.298\n",
      "Loss after 15600 examples: 4.122\n",
      "Loss after 15650 examples: 4.361\n",
      "Loss after 15700 examples: 4.256\n",
      "Loss after 15750 examples: 4.140\n",
      "Loss after 15800 examples: 4.143\n",
      "Loss after 15850 examples: 4.232\n",
      "Loss after 15900 examples: 4.314\n",
      "Loss after 15950 examples: 4.302\n",
      "Loss after 16000 examples: 4.050\n",
      "Loss after 16050 examples: 4.128\n",
      "Loss after 16100 examples: 4.255\n",
      "Loss after 16150 examples: 4.151\n",
      "Loss after 16200 examples: 4.404\n",
      "Loss after 16250 examples: 4.346\n",
      "Loss after 16300 examples: 4.262\n",
      "Loss after 16350 examples: 4.319\n",
      "Loss after 16400 examples: 4.145\n",
      "Loss after 16450 examples: 4.237\n",
      "Loss after 16500 examples: 4.311\n",
      "Loss after 16550 examples: 4.227\n",
      "Loss after 16600 examples: 4.266\n",
      "Loss after 16650 examples: 4.059\n",
      "Loss after 16700 examples: 4.220\n",
      "Loss after 16750 examples: 3.973\n",
      "Loss after 16800 examples: 4.135\n",
      "Loss after 16850 examples: 4.352\n",
      "Loss after 16900 examples: 4.176\n",
      "Loss after 16950 examples: 4.321\n",
      "Loss after 17000 examples: 3.930\n",
      "Loss after 17050 examples: 4.258\n",
      "Loss after 17100 examples: 4.224\n",
      "Loss after 17150 examples: 3.994\n",
      "Loss after 17200 examples: 4.358\n",
      "Loss after 17250 examples: 4.273\n",
      "Loss after 17300 examples: 4.066\n",
      "Loss after 17350 examples: 4.135\n",
      "Loss after 17400 examples: 4.199\n",
      "Loss after 17450 examples: 4.235\n",
      "Loss after 17500 examples: 3.986\n",
      "Loss after 17550 examples: 4.529\n",
      "Loss after 17600 examples: 3.958\n",
      "Loss after 17650 examples: 4.266\n",
      "Loss after 17700 examples: 4.027\n",
      "Loss after 17750 examples: 4.141\n",
      "Loss after 17800 examples: 4.111\n",
      "Loss after 17850 examples: 4.098\n",
      "Loss after 17900 examples: 4.182\n",
      "Loss after 17950 examples: 4.194\n",
      "Loss after 18000 examples: 4.274\n",
      "Loss after 18050 examples: 4.005\n",
      "Loss after 18100 examples: 4.294\n",
      "Loss after 18150 examples: 4.093\n",
      "Loss after 18200 examples: 4.315\n",
      "Loss after 18250 examples: 4.193\n",
      "Loss after 18300 examples: 4.267\n",
      "Loss after 18350 examples: 4.252\n",
      "Loss after 18400 examples: 4.204\n",
      "Loss after 18450 examples: 4.139\n",
      "Loss after 18500 examples: 4.206\n",
      "Loss after 18550 examples: 4.146\n",
      "Loss after 18600 examples: 4.320\n",
      "Loss after 18650 examples: 4.067\n",
      "Loss after 18700 examples: 4.056\n",
      "Loss after 18750 examples: 4.033\n",
      "Loss after 18800 examples: 4.050\n",
      "Loss after 18850 examples: 4.131\n",
      "Loss after 18900 examples: 4.124\n",
      "Loss after 18950 examples: 3.971\n",
      "Loss after 19000 examples: 3.997\n",
      "Loss after 19050 examples: 4.035\n",
      "Loss after 19100 examples: 4.156\n",
      "Loss after 19150 examples: 4.082\n",
      "Loss after 19200 examples: 4.257\n",
      "Loss after 19250 examples: 4.122\n",
      "Loss after 19300 examples: 4.218\n",
      "Loss after 19350 examples: 4.213\n",
      "Loss after 19400 examples: 4.249\n",
      "Loss after 19450 examples: 4.036\n",
      "Loss after 19500 examples: 4.131\n",
      "Loss after 19550 examples: 4.201\n",
      "Loss after 19600 examples: 3.927\n",
      "Loss after 19650 examples: 4.268\n",
      "Loss after 19700 examples: 3.895\n",
      "Loss after 19750 examples: 4.070\n",
      "Loss after 19800 examples: 3.946\n",
      "Loss after 19850 examples: 4.015\n",
      "Loss after 19900 examples: 3.932\n",
      "Loss after 19950 examples: 3.958\n",
      "Loss after 20000 examples: 3.905\n",
      "Loss after 20050 examples: 3.887\n",
      "Loss after 20100 examples: 4.136\n",
      "Loss after 20150 examples: 4.155\n",
      "Loss after 20200 examples: 4.013\n",
      "Loss after 20250 examples: 3.945\n",
      "Loss after 20300 examples: 4.078\n",
      "Loss after 20350 examples: 3.773\n",
      "Loss after 20400 examples: 4.227\n",
      "Loss after 20450 examples: 4.205\n",
      "Loss after 20500 examples: 3.879\n",
      "Loss after 20550 examples: 3.998\n",
      "Loss after 20600 examples: 4.147\n",
      "Loss after 20650 examples: 4.104\n",
      "Loss after 20700 examples: 4.156\n",
      "Loss after 20750 examples: 4.122\n",
      "Loss after 20800 examples: 4.006\n",
      "Loss after 20850 examples: 4.151\n",
      "Loss after 20900 examples: 4.233\n",
      "Loss after 20950 examples: 3.844\n",
      "Loss after 21000 examples: 4.036\n",
      "Loss after 21050 examples: 4.194\n",
      "Loss after 21100 examples: 4.319\n",
      "Loss after 21150 examples: 4.094\n",
      "Loss after 21200 examples: 4.095\n",
      "Loss after 21250 examples: 4.123\n",
      "Loss after 21300 examples: 3.990\n",
      "Loss after 21350 examples: 4.043\n",
      "Loss after 21400 examples: 4.184\n",
      "Loss after 21450 examples: 3.826\n",
      "Loss after 21500 examples: 3.783\n",
      "Loss after 21550 examples: 4.046\n",
      "Loss after 21600 examples: 4.298\n",
      "Loss after 21650 examples: 3.876\n",
      "Loss after 21700 examples: 4.182\n",
      "Loss after 21750 examples: 4.221\n",
      "Loss after 21800 examples: 3.872\n",
      "Loss after 21850 examples: 4.011\n",
      "Loss after 21900 examples: 4.103\n",
      "Loss after 21950 examples: 3.804\n",
      "Loss after 22000 examples: 3.893\n",
      "Loss after 22050 examples: 4.019\n",
      "Loss after 22100 examples: 4.157\n",
      "Loss after 22150 examples: 3.832\n",
      "Loss after 22200 examples: 4.113\n",
      "Loss after 22250 examples: 4.005\n",
      "Loss after 22300 examples: 4.098\n",
      "Loss after 22350 examples: 4.053\n",
      "Loss after 22400 examples: 3.909\n",
      "Loss after 22450 examples: 4.111\n",
      "Loss after 22500 examples: 3.778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11de9470dab9448db6af8f204bd438c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.365 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.003130…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▆▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>3.77775</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-salad-227</strong> at: <a href='https://wandb.ai/grup10/pytorch-demo/runs/abo2hbdy' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/abo2hbdy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230530_130424-abo2hbdy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 164\u001b[0m\n\u001b[0;32m    143\u001b[0m DATA_LOCATION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    145\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    146\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39mDATA_LOCATION\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Images\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    147\u001b[0m     captions_file\u001b[38;5;241m=\u001b[39mDATA_LOCATION\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/captions.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m    162\u001b[0m )\n\u001b[1;32m--> 164\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m, in \u001b[0;36mmodel_pipeline\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 84\u001b[0m     train_loss_arr_aux \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     87\u001b[0m     my_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\train.py:23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, criterion, optimizer, config, epoch, verbatim)\u001b[0m\n\u001b[0;32m     20\u001b[0m example_ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(image)\n\u001b[0;32m     21\u001b[0m batch_ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 23\u001b[0m loss_arr_batch\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Report metrics every 1th batch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((batch_ct \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbatim:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from train import *\n",
    "from test import *\n",
    "from utils.utils import *\n",
    "from models.models import *\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "global device\n",
    "\n",
    "import os\n",
    "\n",
    "# Setting CUDA ALLOC split size to 256 to avoid running out of memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "# Stopping wandb from creating symlinks\n",
    "os.environ[\"WANDB_DISABLE_SYMLINKS\"] = \"true\"\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def model_pipeline(cfg: dict):\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"pytorch-demo\", config=cfg):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # execute only once to create the dataset\n",
    "        # generate_and_dump_dataset(config.root_dir, config.captions_file, config.transforms, cfg.DATA_LOCATION)\n",
    "\n",
    "        # Generate Dataset\n",
    "        dataset = make_dataset(config)\n",
    "\n",
    "        # make the data_loaders, and optimizer\n",
    "        t0 = time.time()\n",
    "        train_loader, test_loader = make_dataloaders(config, dataset, 1)\n",
    "        t1 = time.time()\n",
    "        print(\"Preprocessing_time:\", t1-t0)\n",
    "\n",
    "        # Generate vocab\n",
    "        vocab = dataset.vocab\n",
    "        config.vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "        # Get the model\n",
    "        my_model = make_model(config, device)\n",
    "\n",
    "        # Make the loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "        optimizer = torch.optim.Adam(my_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        train_loss_arr_epoch = []  # Mean of the losses of the last epoch\n",
    "        test_loss_arr_epoch = []\n",
    "        acc_arr_epoch = []\n",
    "\n",
    "        train_loss_arr_batch = [] # Losses of the batches\n",
    "        test_loss_arr_batch = []\n",
    "        acc_arr_batch = []\n",
    "\n",
    "        train_execution_times = []\n",
    "        test_execution_times = []\n",
    "\n",
    "        for epoch in tqdm(range(1, config.epochs + 1)):\n",
    "            # Training the model\n",
    "            t0 = time.time()\n",
    "            train_loss_arr_aux = train(my_model, train_loader, criterion, optimizer, config, epoch)\n",
    "            t1 = time.time()\n",
    "\n",
    "            my_model.eval()\n",
    "            # Testing\n",
    "            t2 = time.time()\n",
    "            acc_arr_aux, test_loss_arr_aux = test(my_model, test_loader, criterion, vocab, config, device)\n",
    "            t3 = time.time()\n",
    "\n",
    "            # Check how model performs\n",
    "            test_model_performance(my_model, test_loader, device, vocab, epoch, config)\n",
    "\n",
    "            my_model.train()\n",
    "\n",
    "            # Logging data for vizz\n",
    "            train_loss_arr_epoch.append(sum(train_loss_arr_aux) / len(train_loss_arr_aux))\n",
    "            test_loss_arr_epoch.append(sum(test_loss_arr_aux) / len(test_loss_arr_aux))\n",
    "\n",
    "            train_loss_arr_batch += train_loss_arr_aux\n",
    "            test_loss_arr_batch += test_loss_arr_aux\n",
    "\n",
    "            acc_arr_epoch.append(sum(acc_arr_aux) / len(acc_arr_aux))\n",
    "            acc_arr_batch += acc_arr_aux\n",
    "\n",
    "            train_execution_times.append(t1-t0)\n",
    "            test_execution_times.append(t3-t2)\n",
    "\n",
    "        epoch_df = pd.DataFrame([train_loss_arr_epoch, test_loss_arr_epoch, acc_arr_epoch, train_execution_times,\n",
    "                                 test_execution_times],\n",
    "                                columns=['epoch_' + str(i) for i in range(len(train_loss_arr_epoch))],\n",
    "                                index=['train_loss', 'test_loss' ,'test_acc', 'train_times','test_times'])\n",
    "        loss_batch_df = pd.DataFrame([train_loss_arr_batch],\n",
    "                                    columns=['batch_' + str(i) for i in range(len(train_loss_arr_batch))],\n",
    "                                    index=['train_loss'])\n",
    "        acc_batch_df = pd.DataFrame([acc_arr_batch, test_loss_arr_batch],\n",
    "                                    columns=['batch_' + str(i) for i in range(len(acc_arr_batch))],\n",
    "                                    index=['test_acc', 'test_loss'])\n",
    "\n",
    "        if config.save:\n",
    "            epoch_df.to_csv(config.DATA_LOCATION+'/logs'+'/epoch_df.csv')\n",
    "            loss_batch_df.to_csv(config.DATA_LOCATION+'/logs'+'/loss_batch_df.csv')\n",
    "            acc_batch_df.to_csv(config.DATA_LOCATION+'/logs'+'/acc_batch_df.csv')\n",
    "            save_model(my_model, config, config.DATA_LOCATION+'/logs'+'/EncoderDecorder_model.pth')\n",
    "\n",
    "    return my_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "\n",
    "    print(\"Using: \", device)\n",
    "\n",
    "    transforms = T.Compose([\n",
    "        T.Resize(226),\n",
    "        T.RandomCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    DATA_LOCATION = '../data'\n",
    "\n",
    "    config = dict(\n",
    "        root_dir=DATA_LOCATION+\"/Images\",\n",
    "        captions_file=DATA_LOCATION+\"/captions.txt\",\n",
    "        device=device,\n",
    "        encoder='ResNet50',\n",
    "        transforms=transforms,\n",
    "        embed_size=300,\n",
    "        attention_dim=256,\n",
    "        encoder_dim=2048,\n",
    "        decoder_dim=512,\n",
    "        epochs=20,\n",
    "        learning_rate=0.0001,\n",
    "        batch_size=50,\n",
    "        DATA_LOCATION=DATA_LOCATION,\n",
    "        train_size=0.8,\n",
    "        save=True,\n",
    "        momentum=0.8\n",
    "    )\n",
    "\n",
    "    model = model_pipeline(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa0e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b48135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2e495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371f04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fdc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f6c010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Pau\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\wandb\\run-20230530_131140-t9rs5a0n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n' target=\"_blank\">deep-violet-228</a></strong> to <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-violet-228</strong> at: <a href='https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230530_131140-t9rs5a0n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"pytorch-demo\", config=config):\n",
    "    config = wandb.config\n",
    "\n",
    "    # Generate Dataset\n",
    "    dataset = make_dataset(config)\n",
    "\n",
    "    # make the data_loaders, and optimizer\n",
    "    #train_loader, test_loader = make_dataloaders(config, dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ee7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3257c262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[[-0.9019, -0.1315, -0.2341,  ..., -1.3301, -1.5527, -1.2783],\n",
       "           [-1.0215, -0.1829, -0.2170,  ..., -1.4502, -1.5869, -1.6895],\n",
       "           [-1.0566, -0.1656, -0.2000,  ..., -1.6211, -1.5527, -1.6387],\n",
       "           ...,\n",
       "           [ 1.6670,  1.4951,  0.4680,  ...,  1.5293,  0.6733,  0.7246],\n",
       "           [ 0.8789,  0.1083,  0.0056,  ...,  1.4951,  0.6733,  0.6904],\n",
       "           [ 0.6904,  1.5469,  0.9644,  ...,  1.4951,  0.7075,  0.6904]],\n",
       "  \n",
       "          [[-0.7925,  0.0301, -0.0049,  ..., -1.2832, -1.5107, -0.9502],\n",
       "           [-0.9678, -0.0224,  0.0476,  ..., -1.4229, -1.5635, -1.5801],\n",
       "           [-1.0029, -0.0049,  0.0301,  ..., -1.5459, -1.4404, -1.5459],\n",
       "           ...,\n",
       "           [ 1.2207,  0.7656, -0.6177,  ...,  1.8506,  1.2031,  1.1855],\n",
       "           [-0.2500, -0.4775, -0.4602,  ...,  1.8330,  1.2207,  1.2031],\n",
       "           [ 0.0476,  1.0459,  0.3103,  ...,  1.8154,  1.2031,  1.1855]],\n",
       "  \n",
       "          [[-0.6021,  0.1302,  0.0779,  ..., -1.2812, -1.4902, -1.2637],\n",
       "           [-0.6890,  0.0779,  0.0605,  ..., -1.4033, -1.4902, -1.5781],\n",
       "           [-0.7935,  0.0953,  0.1302,  ..., -1.5254, -1.4033, -1.5781],\n",
       "           ...,\n",
       "           [-0.2358, -0.4275, -1.2295,  ...,  2.3086,  1.8906,  1.7861],\n",
       "           [-1.0547, -1.0898, -1.0898,  ...,  2.2910,  1.8906,  1.7686],\n",
       "           [-0.6714, -0.2358, -0.7759,  ...,  2.2734,  1.8555,  1.7510]]],\n",
       "         dtype=torch.float16),\n",
       "  [tensor([  1,   4,  28,   8,   4, 195, 151,  17,  32,  67,   4, 353,  11, 711,\n",
       "             8,  24,   3, 496,   5,   2]),\n",
       "   tensor([  1,   4,   7, 316,  76,   4, 157,  74,   5,   2]),\n",
       "   tensor([   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2]),\n",
       "   tensor([   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2]),\n",
       "   tensor([  1,   4,   9,   7,   8,   4, 195, 151, 316,  76,   4, 157,   3,   5,\n",
       "             2])]],\n",
       " [tensor([[[-1.4160, -1.4160, -1.4160,  ..., -1.1758, -1.2959, -1.2959],\n",
       "           [-1.4326, -1.4326, -1.4160,  ..., -1.3301, -1.3135, -1.2441],\n",
       "           [-1.4502, -1.4326, -1.4502,  ..., -1.5527, -1.4844, -1.2275],\n",
       "           ...,\n",
       "           [ 0.1940,  0.1255,  0.3652,  ..., -0.4397, -0.4739, -0.2341],\n",
       "           [ 0.1426,  0.2281,  0.1768,  ..., -0.1486, -0.1486, -0.2000],\n",
       "           [ 0.2111,  0.0398, -0.1656,  ..., -0.1143, -0.3540, -0.4910]],\n",
       "  \n",
       "          [[-1.3525, -1.3525, -1.3525,  ..., -0.6177, -0.7925, -0.8804],\n",
       "           [-1.3701, -1.3701, -1.3525,  ..., -0.8804, -0.8804, -0.8452],\n",
       "           [-1.3877, -1.3701, -1.3877,  ..., -1.2129, -1.1426, -0.8804],\n",
       "           ...,\n",
       "           [ 0.2927,  0.2227,  0.4502,  ..., -0.3550, -0.3901, -0.1449],\n",
       "           [ 0.2052,  0.3276,  0.2578,  ..., -0.0399, -0.0399, -0.0924],\n",
       "           [ 0.3103,  0.1876, -0.0399,  ..., -0.0049, -0.2676, -0.3726]],\n",
       "  \n",
       "          [[-1.1074, -1.1074, -1.1074,  ..., -1.1592, -1.2812, -1.3164],\n",
       "           [-1.1250, -1.1250, -1.1074,  ..., -1.2119, -1.1943, -1.2471],\n",
       "           [-1.1426, -1.1250, -1.1426,  ..., -1.2988, -1.2295, -1.1943],\n",
       "           ...,\n",
       "           [ 0.5835,  0.5137,  0.7056,  ...,  0.0779,  0.0082,  0.2522],\n",
       "           [ 0.5312,  0.6528,  0.5483,  ...,  0.2871,  0.3044,  0.2871],\n",
       "           [ 0.6006,  0.4961,  0.2695,  ...,  0.3394,  0.1128,  0.0256]]],\n",
       "         dtype=torch.float16),\n",
       "  [tensor([  1,   4,  20,   6,  16,   4, 680,   6,  34, 694,   2]),\n",
       "   tensor([  1,   4,  20,   6,  16,   4, 898, 116, 239,   6,  41,  12, 106, 107,\n",
       "            13,  10, 177,   5,   2]),\n",
       "   tensor([  1,   4,  20,   6,  16,   4,  21,   6,  12,  30, 930,  34, 837,  19,\n",
       "           106, 107,   8,  10,  65,   5,   2]),\n",
       "   tensor([   1,   50,   51,   11,  321, 1904,   78,   19,  106,  107,   13,   10,\n",
       "            177,    5,    2]),\n",
       "   tensor([  1,  50,  51,  13, 592, 622, 544, 106, 107,   5,   2])]],\n",
       " [tensor([[[ 1.4951,  1.5127,  1.5127,  ...,  1.7012,  1.7178,  1.7012],\n",
       "           [ 1.4443,  1.4609,  1.4609,  ...,  1.7178,  1.7178,  1.7012],\n",
       "           [ 1.3926,  1.4268,  1.4443,  ...,  1.7178,  1.7012,  1.6836],\n",
       "           ...,\n",
       "           [-0.5596, -1.0908, -1.0732,  ..., -0.5938, -0.5254, -0.3198],\n",
       "           [-0.2000, -0.9707, -1.0215,  ..., -0.4568, -0.6792, -0.2341],\n",
       "           [-0.5767, -0.5767, -0.9878,  ..., -0.4397, -0.8335, -0.1829]],\n",
       "  \n",
       "          [[ 1.7812,  1.7979,  1.7979,  ...,  1.9736,  1.9912,  1.9912],\n",
       "           [ 1.7285,  1.7461,  1.7461,  ...,  1.9912,  1.9912,  2.0254],\n",
       "           [ 1.7109,  1.7109,  1.7285,  ...,  1.9912,  1.9912,  2.0078],\n",
       "           ...,\n",
       "           [ 0.0476, -0.5479, -0.4951,  ..., -0.0224, -0.0049,  0.2052],\n",
       "           [ 0.2227, -0.4951, -0.3376,  ...,  0.1702, -0.0750,  0.2751],\n",
       "           [-0.1274, -0.0750, -0.3726,  ...,  0.0476, -0.1799,  0.3452]],\n",
       "  \n",
       "          [[ 1.8730,  1.8906,  1.8906,  ...,  2.1523,  2.1699,  2.1699],\n",
       "           [ 1.8213,  1.8379,  1.8379,  ...,  2.1699,  2.1523,  2.1523],\n",
       "           [ 1.7686,  1.7861,  1.8037,  ...,  2.1699,  2.1699,  2.1699],\n",
       "           ...,\n",
       "           [-0.7588, -1.2637, -1.0723,  ..., -1.1426, -1.0205, -1.0029],\n",
       "           [-0.5845, -1.1592, -1.1943,  ..., -0.7588, -1.0723, -0.6890],\n",
       "           [-0.8633, -0.9331, -1.1426,  ..., -0.9678, -1.3691, -0.5146]]],\n",
       "         dtype=torch.float16),\n",
       "  [tensor([   1,    4,    9,    7,  114,    8, 1447,   77,    8,   23,   11,    4,\n",
       "            325,  643,   12,  104,  229,    8,    4, 1301,    5,    2]),\n",
       "   tensor([  1,   4,   9,   7,  17,  46,   8,  23,  11,   4,  53, 325, 643,   5,\n",
       "             2]),\n",
       "   tensor([   1,    4,   85,    7,    8,   10,   22,   92,   12,    3,    8,   23,\n",
       "             11,    4,   21, 2306,   12,    4,  643,   13,   64,    5,    2]),\n",
       "   tensor([  1, 204,  17,   4,   7,  12,  91,  46,   8,  23,  11,   4, 643, 176,\n",
       "             5,   2]),\n",
       "   tensor([  1,  61,   7,  12,  91, 176,  70,   8,  10,  22,   5,   2])]]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a0600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f98ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0b2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb01245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iter = iter(train_loader)\n",
    "img, cap = next(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7397267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935a423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f8f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2, cap2 = next(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34828683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff59b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25206cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f300718",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,captions = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99653d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, captions = image.to(device), captions.to(device)\n",
    "\n",
    "# Zero the gradients.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Feed forward\n",
    "outputs, attentions = my_model(image.to(torch.float32), captions)\n",
    "\n",
    "# Calculate the batch loss.\n",
    "targets = captions[:, 1:]\n",
    "loss = criterion(outputs.view(-1, config.vocab_size), targets.reshape(-1))\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Update the parameters in the optimizer.\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69795396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0419, -0.0200, -0.0398,  ..., -0.0310,  0.0729,  0.0062],\n",
       "        [ 0.1156, -0.0232, -0.1058,  ...,  0.0588,  0.0166, -0.1509],\n",
       "        [ 0.0336,  0.0681, -0.1028,  ...,  0.0521,  0.0040, -0.1283],\n",
       "        ...,\n",
       "        [-0.2936,  0.1309,  0.0571,  ..., -0.1252,  0.2291, -0.0856],\n",
       "        [-0.2734,  0.2516, -0.0050,  ...,  0.0603, -0.0179, -0.0015],\n",
       "        [-0.2988,  0.1344, -0.0523,  ...,  0.0208, -0.0471,  0.0206]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.view(-1, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac22fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 20])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41efcc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0419, -0.0200, -0.0398,  ..., -0.0310,  0.0729,  0.0062],\n",
       "        [ 0.1156, -0.0232, -0.1058,  ...,  0.0588,  0.0166, -0.1509],\n",
       "        [ 0.0336,  0.0681, -0.1028,  ...,  0.0521,  0.0040, -0.1283],\n",
       "        ...,\n",
       "        [-0.2936,  0.1309,  0.0571,  ..., -0.1252,  0.2291, -0.0856],\n",
       "        [-0.2734,  0.2516, -0.0050,  ...,  0.0603, -0.0179, -0.0015],\n",
       "        [-0.2988,  0.1344, -0.0523,  ...,  0.0208, -0.0471,  0.0206]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.view(-1, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training and track with wandb\n",
    "example_ct = 0  # number of examples seen\n",
    "batch_ct = 0\n",
    "\n",
    "loss_arr_batch = []  # Losses of the batches\n",
    "\n",
    "for idx, (image, captions) in enumerate(iter(data_loader)):\n",
    "\n",
    "    loss = train_batch(image.to(torch.float32), captions, model, config.vocab_size, optimizer, criterion, device=config.device)\n",
    "    example_ct += len(image)\n",
    "    batch_ct += 1\n",
    "\n",
    "    loss_arr_batch.append(loss.tolist())\n",
    "\n",
    "    # Report metrics every 1th batch\n",
    "    if ((batch_ct + 1) % 1) == 0 and verbatim:\n",
    "        train_log(loss, example_ct, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba8be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41984f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b5b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28def3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71b03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c40f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc53be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad055988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e516d6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.296547889709473"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "my_iter = iter(train_loader)\n",
    "t1 = time.time()\n",
    "t0-t1\n",
    "# bs 32 nw all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae5e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d98d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   4,  28,   8,   4, 195, 151,  17,  32,  67,   4, 353,  11, 711,\n",
       "          8,  24,   3, 496,   5,   2], dtype=torch.int16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data[0//5][1][0%5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e378e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb7d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70173f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e5986ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.776714086532593"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "my_iter = iter(train_loader)\n",
    "t1 = time.time()\n",
    "t0-t1\n",
    "# bs 500 nw 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de2c3f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for a, b in my_iter:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2e5435a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "('{} cannot be pickled', '_MultiProcessingDataLoaderIter')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iter_2 \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\copy.py:161\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    159\u001b[0m reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43mreductor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:657\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;66;03m# TODO: add limited pickling support for sharing an iterator\u001b[39;00m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;66;03m# across multiple threads for HOGWILD.\u001b[39;00m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;66;03m# Probably the best way to do this is by moving the sample pushing\u001b[39;00m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# to a separate thread and then just sharing the data queue\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;66;03m# but signalling the end is tricky without a non-blocking API\u001b[39;00m\n\u001b[1;32m--> 657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be pickled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ('{} cannot be pickled', '_MultiProcessingDataLoaderIter')"
     ]
    }
   ],
   "source": [
    "iter_2 = deepcopy(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eef0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xnap-example",
   "language": "python",
   "name": "xnap-example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
