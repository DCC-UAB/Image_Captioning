{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a69d37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Pau\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\wandb\\run-20230529_193209-fkexkv9q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grup10/pytorch-demo/runs/fkexkv9q' target=\"_blank\">decent-sky-189</a></strong> to <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grup10/pytorch-demo/runs/fkexkv9q' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/fkexkv9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing_time: 9.421124935150146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229834afa4b34023ba09bae391e85ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00030 examples: 8.016\n",
      "Loss after 00060 examples: 8.020\n",
      "Loss after 00090 examples: 8.006\n",
      "Loss after 00120 examples: 7.980\n",
      "Loss after 00150 examples: 7.966\n",
      "Loss after 00180 examples: 7.950\n",
      "Loss after 00210 examples: 7.923\n",
      "Loss after 00240 examples: 7.891\n",
      "Loss after 00270 examples: 7.880\n",
      "Loss after 00300 examples: 7.844\n",
      "Loss after 00330 examples: 7.800\n",
      "Loss after 00360 examples: 7.782\n",
      "Loss after 00390 examples: 7.740\n",
      "Loss after 00420 examples: 7.729\n",
      "Loss after 00450 examples: 7.697\n",
      "Loss after 00480 examples: 7.657\n",
      "Loss after 00510 examples: 7.625\n",
      "Loss after 00540 examples: 7.586\n",
      "Loss after 00570 examples: 7.552\n",
      "Loss after 00600 examples: 7.505\n",
      "Loss after 00630 examples: 7.476\n",
      "Loss after 00660 examples: 7.460\n",
      "Loss after 00690 examples: 7.420\n",
      "Loss after 00720 examples: 7.355\n",
      "Loss after 00750 examples: 7.331\n",
      "Loss after 00780 examples: 7.226\n",
      "Loss after 00810 examples: 7.227\n",
      "Loss after 00840 examples: 7.232\n",
      "Loss after 00870 examples: 7.125\n",
      "Loss after 00900 examples: 7.069\n",
      "Loss after 00930 examples: 7.016\n",
      "Loss after 00960 examples: 6.909\n",
      "Loss after 00990 examples: 6.917\n",
      "Loss after 01020 examples: 6.882\n",
      "Loss after 01050 examples: 6.721\n",
      "Loss after 01080 examples: 6.729\n",
      "Loss after 01110 examples: 6.646\n",
      "Loss after 01140 examples: 6.482\n",
      "Loss after 01170 examples: 6.326\n",
      "Loss after 01200 examples: 6.340\n",
      "Loss after 01230 examples: 6.372\n",
      "Loss after 01260 examples: 6.248\n",
      "Loss after 01290 examples: 6.111\n",
      "Loss after 01320 examples: 5.880\n",
      "Loss after 01350 examples: 5.884\n",
      "Loss after 01380 examples: 5.864\n",
      "Loss after 01410 examples: 5.736\n",
      "Loss after 01440 examples: 5.605\n",
      "Loss after 01470 examples: 5.426\n",
      "Loss after 01500 examples: 5.438\n",
      "Loss after 01530 examples: 5.191\n",
      "Loss after 01560 examples: 5.287\n",
      "Loss after 01590 examples: 5.125\n",
      "Loss after 01620 examples: 4.911\n",
      "Loss after 01650 examples: 4.903\n",
      "Loss after 01680 examples: 4.827\n",
      "Loss after 01710 examples: 4.573\n",
      "Loss after 01740 examples: 4.569\n",
      "Loss after 01770 examples: 4.264\n",
      "Loss after 01800 examples: 4.182\n",
      "Loss after 01830 examples: 4.001\n",
      "Loss after 01860 examples: 4.290\n",
      "Loss after 01890 examples: 4.015\n",
      "Loss after 01920 examples: 3.730\n",
      "Loss after 01950 examples: 4.171\n",
      "Loss after 01980 examples: 3.748\n",
      "Loss after 02010 examples: 3.839\n",
      "Loss after 02040 examples: 3.658\n",
      "Loss after 02070 examples: 3.468\n",
      "Loss after 02100 examples: 3.302\n",
      "Loss after 02130 examples: 3.714\n",
      "Loss after 02160 examples: 3.473\n",
      "Loss after 02190 examples: 3.549\n",
      "Loss after 02220 examples: 3.448\n",
      "Loss after 02250 examples: 3.437\n",
      "Loss after 02280 examples: 3.153\n",
      "Loss after 02310 examples: 3.237\n",
      "Loss after 02340 examples: 3.254\n",
      "Loss after 02370 examples: 3.095\n",
      "Loss after 02400 examples: 3.370\n",
      "Loss after 02430 examples: 3.139\n",
      "Loss after 02460 examples: 3.020\n",
      "Loss after 02490 examples: 2.930\n",
      "Loss after 02520 examples: 3.030\n",
      "Loss after 02550 examples: 3.095\n",
      "Loss after 02580 examples: 2.588\n",
      "Loss after 02610 examples: 2.615\n",
      "Loss after 02640 examples: 2.646\n",
      "Loss after 02670 examples: 2.800\n",
      "Loss after 02700 examples: 2.588\n",
      "Loss after 02730 examples: 2.613\n",
      "Loss after 02760 examples: 2.359\n",
      "Loss after 02790 examples: 2.456\n",
      "Loss after 02820 examples: 2.304\n",
      "Loss after 02850 examples: 2.567\n",
      "Loss after 02880 examples: 2.549\n",
      "Loss after 02910 examples: 2.327\n",
      "Loss after 02940 examples: 2.171\n",
      "Loss after 02970 examples: 2.173\n",
      "Loss after 03000 examples: 2.277\n",
      "Loss after 03030 examples: 2.212\n",
      "Loss after 03060 examples: 2.224\n",
      "Loss after 03090 examples: 2.332\n",
      "Loss after 03120 examples: 2.279\n",
      "Loss after 03150 examples: 2.170\n",
      "Loss after 03180 examples: 2.213\n",
      "Loss after 03210 examples: 1.862\n",
      "Loss after 03240 examples: 2.109\n",
      "Loss after 03270 examples: 2.369\n",
      "Loss after 03300 examples: 2.180\n",
      "Loss after 03330 examples: 2.019\n",
      "Loss after 03360 examples: 2.225\n",
      "Loss after 03390 examples: 1.950\n",
      "Loss after 03420 examples: 2.025\n",
      "Loss after 03450 examples: 1.736\n",
      "Loss after 03480 examples: 1.878\n",
      "Loss after 03510 examples: 2.037\n",
      "Loss after 03540 examples: 1.834\n",
      "Loss after 03570 examples: 1.849\n",
      "Loss after 03600 examples: 1.972\n",
      "Loss after 03630 examples: 1.719\n",
      "Loss after 03660 examples: 1.776\n",
      "Loss after 03690 examples: 1.567\n",
      "Loss after 03720 examples: 1.937\n",
      "Loss after 03750 examples: 1.626\n",
      "Loss after 03780 examples: 1.706\n",
      "Loss after 03810 examples: 1.655\n",
      "Loss after 03840 examples: 1.680\n",
      "Loss after 03870 examples: 1.542\n",
      "Loss after 03900 examples: 1.631\n",
      "Loss after 03930 examples: 1.545\n",
      "Loss after 03960 examples: 1.587\n",
      "Loss after 03990 examples: 1.531\n",
      "Loss after 04020 examples: 1.542\n",
      "Loss after 04045 examples: 1.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pau\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Pau\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU score of the model on the 100 test images: 6.030654703641959e-155%\n",
      "count: 2.415217638015747\n",
      "Loss after 00030 examples: 1.572\n",
      "Loss after 00060 examples: 1.505\n",
      "Loss after 00090 examples: 1.460\n",
      "Loss after 00120 examples: 1.395\n",
      "Loss after 00150 examples: 1.386\n",
      "Loss after 00180 examples: 1.405\n",
      "Loss after 00210 examples: 1.446\n",
      "Loss after 00240 examples: 1.515\n",
      "Loss after 00270 examples: 1.421\n",
      "Loss after 00300 examples: 1.243\n",
      "Loss after 00330 examples: 1.493\n",
      "Loss after 00360 examples: 1.356\n",
      "Loss after 00390 examples: 1.218\n",
      "Loss after 00420 examples: 1.446\n",
      "Loss after 00450 examples: 1.384\n",
      "Loss after 00480 examples: 1.197\n",
      "Loss after 00510 examples: 1.217\n",
      "Loss after 00540 examples: 1.167\n",
      "Loss after 00570 examples: 1.338\n",
      "Loss after 00600 examples: 1.153\n",
      "Loss after 00630 examples: 1.095\n",
      "Loss after 00660 examples: 1.124\n",
      "Loss after 00690 examples: 1.139\n",
      "Loss after 00720 examples: 1.201\n",
      "Loss after 00750 examples: 1.048\n",
      "Loss after 00780 examples: 1.164\n",
      "Loss after 00810 examples: 1.274\n",
      "Loss after 00840 examples: 1.145\n",
      "Loss after 00870 examples: 1.040\n",
      "Loss after 00900 examples: 1.093\n",
      "Loss after 00930 examples: 1.174\n",
      "Loss after 00960 examples: 1.187\n",
      "Loss after 00990 examples: 1.079\n",
      "Loss after 01020 examples: 1.069\n",
      "Loss after 01050 examples: 1.153\n",
      "Loss after 01080 examples: 1.081\n",
      "Loss after 01110 examples: 0.965\n",
      "Loss after 01140 examples: 0.894\n",
      "Loss after 01170 examples: 0.947\n",
      "Loss after 01200 examples: 1.017\n",
      "Loss after 01230 examples: 1.059\n",
      "Loss after 01260 examples: 0.982\n",
      "Loss after 01290 examples: 1.052\n",
      "Loss after 01320 examples: 0.863\n",
      "Loss after 01350 examples: 0.884\n",
      "Loss after 01380 examples: 1.173\n",
      "Loss after 01410 examples: 0.872\n",
      "Loss after 01440 examples: 0.967\n",
      "Loss after 01470 examples: 0.958\n",
      "Loss after 01500 examples: 0.946\n",
      "Loss after 01530 examples: 0.905\n",
      "Loss after 01560 examples: 0.891\n",
      "Loss after 01590 examples: 0.834\n",
      "Loss after 01620 examples: 0.916\n",
      "Loss after 01650 examples: 0.915\n",
      "Loss after 01680 examples: 0.913\n",
      "Loss after 01710 examples: 0.981\n",
      "Loss after 01740 examples: 0.806\n",
      "Loss after 01770 examples: 0.841\n",
      "Loss after 01800 examples: 0.847\n",
      "Loss after 01830 examples: 0.943\n",
      "Loss after 01860 examples: 0.944\n",
      "Loss after 01890 examples: 0.799\n",
      "Loss after 01920 examples: 0.824\n",
      "Loss after 01950 examples: 0.830\n",
      "Loss after 01980 examples: 0.807\n",
      "Loss after 02010 examples: 0.892\n",
      "Loss after 02040 examples: 0.772\n",
      "Loss after 02070 examples: 0.726\n",
      "Loss after 02100 examples: 0.799\n",
      "Loss after 02130 examples: 0.744\n",
      "Loss after 02160 examples: 0.777\n",
      "Loss after 02190 examples: 0.700\n",
      "Loss after 02220 examples: 0.786\n",
      "Loss after 02250 examples: 0.736\n",
      "Loss after 02280 examples: 0.756\n",
      "Loss after 02310 examples: 0.777\n",
      "Loss after 02340 examples: 0.734\n",
      "Loss after 02370 examples: 0.709\n",
      "Loss after 02400 examples: 0.763\n",
      "Loss after 02430 examples: 0.695\n",
      "Loss after 02460 examples: 0.700\n",
      "Loss after 02490 examples: 0.751\n",
      "Loss after 02520 examples: 0.681\n",
      "Loss after 02550 examples: 0.672\n",
      "Loss after 02580 examples: 0.694\n",
      "Loss after 02610 examples: 0.712\n",
      "Loss after 02640 examples: 0.675\n",
      "Loss after 02670 examples: 0.700\n",
      "Loss after 02700 examples: 0.678\n",
      "Loss after 02730 examples: 0.614\n",
      "Loss after 02760 examples: 0.675\n",
      "Loss after 02790 examples: 0.639\n",
      "Loss after 02820 examples: 0.625\n",
      "Loss after 02850 examples: 0.599\n",
      "Loss after 02880 examples: 0.627\n",
      "Loss after 02910 examples: 0.625\n",
      "Loss after 02940 examples: 0.605\n",
      "Loss after 02970 examples: 0.657\n",
      "Loss after 03000 examples: 0.569\n",
      "Loss after 03030 examples: 0.571\n",
      "Loss after 03060 examples: 0.570\n",
      "Loss after 03090 examples: 0.586\n",
      "Loss after 03120 examples: 0.595\n",
      "Loss after 03150 examples: 0.584\n",
      "Loss after 03180 examples: 0.541\n",
      "Loss after 03210 examples: 0.585\n",
      "Loss after 03240 examples: 0.617\n",
      "Loss after 03270 examples: 0.563\n",
      "Loss after 03300 examples: 0.542\n",
      "Loss after 03330 examples: 0.593\n",
      "Loss after 03360 examples: 0.553\n",
      "Loss after 03390 examples: 0.600\n",
      "Loss after 03420 examples: 0.539\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>██████▇▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>test_mean_bleu</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>1.5289</td></tr><tr><td>test_mean_bleu</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sky-189</strong> at: <a href='https://wandb.ai/grup10/pytorch-demo/runs/fkexkv9q' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/fkexkv9q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230529_193209-fkexkv9q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 164\u001b[0m\n\u001b[0;32m    143\u001b[0m DATA_LOCATION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    145\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    146\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39mDATA_LOCATION\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Images\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    147\u001b[0m     captions_file\u001b[38;5;241m=\u001b[39mDATA_LOCATION\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/captions.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m    162\u001b[0m )\n\u001b[1;32m--> 164\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m, in \u001b[0;36mmodel_pipeline\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 84\u001b[0m     train_loss_arr_aux \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     87\u001b[0m     my_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\train.py:19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, criterion, optimizer, config, epoch, verbatim)\u001b[0m\n\u001b[0;32m     15\u001b[0m loss_arr_batch \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Losses of the batches\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (image, captions) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_loader)):\n\u001b[1;32m---> 19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     example_ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(image)\n\u001b[0;32m     21\u001b[0m     batch_ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\train.py:40\u001b[0m, in \u001b[0;36mtrain_batch\u001b[1;34m(image, captions, model, vocab_size, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Feed forward\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m outputs, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Calculate the batch loss.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m targets \u001b[38;5;241m=\u001b[39m captions[:, \u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\wandb_torch.py:110\u001b[0m, in \u001b[0;36mTorchHistory.add_log_parameters_hook.<locals>.<lambda>\u001b[1;34m(mod, inp, outp)\u001b[0m\n\u001b[0;32m    107\u001b[0m log_track_params \u001b[38;5;241m=\u001b[39m log_track_init(log_freq)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     hook \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mregister_forward_hook(\n\u001b[1;32m--> 110\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m mod, inp, outp: \u001b[43mparameter_log_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_track_params\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_handles[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix] \u001b[38;5;241m=\u001b[39m hook\n\u001b[0;32m    115\u001b[0m     module\u001b[38;5;241m.\u001b[39m_wandb_hook_names\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\wandb_torch.py:105\u001b[0m, in \u001b[0;36mTorchHistory.add_log_parameters_hook.<locals>.parameter_log_hook\u001b[1;34m(module, input_, output, log_track)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     data \u001b[38;5;241m=\u001b[39m parameter\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_tensor_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\wandb_torch.py:256\u001b[0m, in \u001b[0;36mTorchHistory.log_tensor_stats\u001b[1;34m(self, tensor, name)\u001b[0m\n\u001b[0;32m    253\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(tensor_np)\n\u001b[0;32m    254\u001b[0m     bins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(bins_np)\n\u001b[1;32m--> 256\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_histogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:1532\u001b[0m, in \u001b[0;36mRun._log\u001b[1;34m(self, data, step, commit)\u001b[0m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey values passed to `wandb.log` must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1532\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_history_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetpid() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pid \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attached:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:1402\u001b[0m, in \u001b[0;36mRun._partial_history_callback\u001b[1;34m(self, row, step, commit)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface:\n\u001b[0;32m   1400\u001b[0m     not_using_tensorboard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(wandb\u001b[38;5;241m.\u001b[39mpatched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1402\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_partial_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublish_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_using_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py:585\u001b[0m, in \u001b[0;36mInterfaceBase.publish_partial_history\u001b[1;34m(self, data, user_step, step, flush, publish_step, run)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flush \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    584\u001b[0m     partial_history\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mflush \u001b[38;5;241m=\u001b[39m flush\n\u001b[1;32m--> 585\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_partial_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:89\u001b[0m, in \u001b[0;36mInterfaceShared._publish_partial_history\u001b[1;34m(self, partial_history)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_partial_history\u001b[39m(\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m, partial_history: pb\u001b[38;5;241m.\u001b[39mPartialHistoryRequest\n\u001b[0;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(partial_history\u001b[38;5;241m=\u001b[39mpartial_history)\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[1;34m(self, record, local)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:221\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    219\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[0;32m    220\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    150\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from train import *\n",
    "from test import *\n",
    "from utils.utils import *\n",
    "from models.models import *\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "global device\n",
    "\n",
    "import os\n",
    "\n",
    "# Setting CUDA ALLOC split size to 256 to avoid running out of memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "# Stopping wandb from creating symlinks\n",
    "os.environ[\"WANDB_DISABLE_SYMLINKS\"] = \"true\"\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def model_pipeline(cfg: dict):\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"pytorch-demo\", config=cfg):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # execute only once to create the dataset\n",
    "        # generate_and_dump_dataset(config.root_dir, config.captions_file, config.transforms, cfg.DATA_LOCATION)\n",
    "\n",
    "        # Generate Dataset\n",
    "        dataset = make_dataset(config)\n",
    "\n",
    "        # make the data_loaders, and optimizer\n",
    "        t0 = time.time()\n",
    "        train_loader, test_loader = make_dataloaders(config, dataset, 1)\n",
    "        t1 = time.time()\n",
    "        print(\"Preprocessing_time:\", t1-t0)\n",
    "\n",
    "        # Generate vocab\n",
    "        vocab = dataset.vocab\n",
    "        config.vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "        # Get the model\n",
    "        my_model = make_model(config, device)\n",
    "\n",
    "        # Make the loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "        optimizer = torch.optim.SGD(my_model.parameters(), lr=config.learning_rate, momentum=config.momentum)\n",
    "\n",
    "        train_loss_arr_epoch = []  # Mean of the losses of the last epoch\n",
    "        test_loss_arr_epoch = []\n",
    "        acc_arr_epoch = []\n",
    "\n",
    "        train_loss_arr_batch = [] # Losses of the batches\n",
    "        test_loss_arr_batch = []\n",
    "        acc_arr_batch = []\n",
    "\n",
    "        train_execution_times = []\n",
    "        test_execution_times = []\n",
    "\n",
    "        for epoch in tqdm(range(1, config.epochs + 1)):\n",
    "            # Training the model\n",
    "            t0 = time.time()\n",
    "            train_loss_arr_aux = train(my_model, train_loader, criterion, optimizer, config, epoch)\n",
    "            t1 = time.time()\n",
    "\n",
    "            my_model.eval()\n",
    "            # Testing\n",
    "            t2 = time.time()\n",
    "            acc_arr_aux, test_loss_arr_aux = test(my_model, test_loader, criterion, vocab, config, device)\n",
    "            t3 = time.time()\n",
    "\n",
    "            # Check how model performs\n",
    "            test_model_performance(my_model, test_loader, device, vocab, epoch, config)\n",
    "\n",
    "            my_model.train()\n",
    "\n",
    "            # Logging data for vizz\n",
    "            train_loss_arr_epoch.append(sum(train_loss_arr_aux) / len(train_loss_arr_aux))\n",
    "            test_loss_arr_epoch.append(sum(test_loss_arr_aux) / len(test_loss_arr_aux))\n",
    "\n",
    "            train_loss_arr_batch += train_loss_arr_aux\n",
    "            test_loss_arr_batch += test_loss_arr_aux\n",
    "\n",
    "            acc_arr_epoch.append(sum(acc_arr_aux) / len(acc_arr_aux))\n",
    "            acc_arr_batch += acc_arr_aux\n",
    "\n",
    "            train_execution_times.append(t1-t0)\n",
    "            test_execution_times.append(t3-t2)\n",
    "\n",
    "        epoch_df = pd.DataFrame([train_loss_arr_epoch, test_loss_arr_epoch, acc_arr_epoch, train_execution_times,\n",
    "                                 test_execution_times],\n",
    "                                columns=['epoch_' + str(i) for i in range(len(train_loss_arr_epoch))],\n",
    "                                index=['train_loss', 'test_loss' ,'test_acc', 'train_times','test_times'])\n",
    "        loss_batch_df = pd.DataFrame([train_loss_arr_batch],\n",
    "                                    columns=['batch_' + str(i) for i in range(len(train_loss_arr_batch))],\n",
    "                                    index=['train_loss'])\n",
    "        acc_batch_df = pd.DataFrame([acc_arr_batch, test_loss_arr_batch],\n",
    "                                    columns=['batch_' + str(i) for i in range(len(acc_arr_batch))],\n",
    "                                    index=['test_acc', 'test_loss'])\n",
    "\n",
    "        if config.save:\n",
    "            epoch_df.to_csv(config.DATA_LOCATION+'/logs'+'/epoch_df.csv')\n",
    "            loss_batch_df.to_csv(config.DATA_LOCATION+'/logs'+'/loss_batch_df.csv')\n",
    "            acc_batch_df.to_csv(config.DATA_LOCATION+'/logs'+'/acc_batch_df.csv')\n",
    "            save_model(my_model, config, config.DATA_LOCATION+'/logs'+'/EncoderDecorder_model.pth')\n",
    "\n",
    "    return my_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "\n",
    "    print(\"Using: \", device)\n",
    "\n",
    "    transforms = T.Compose([\n",
    "        T.Resize(226),\n",
    "        T.RandomCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    DATA_LOCATION = '../data'\n",
    "\n",
    "    config = dict(\n",
    "        root_dir=DATA_LOCATION+\"/Images\",\n",
    "        captions_file=DATA_LOCATION+\"/captions.txt\",\n",
    "        device=device,\n",
    "        encoder='ResNet50',\n",
    "        transforms=transforms,\n",
    "        embed_size=300,\n",
    "        attention_dim=256,\n",
    "        encoder_dim=2048,\n",
    "        decoder_dim=512,\n",
    "        epochs=20,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=30,\n",
    "        DATA_LOCATION=DATA_LOCATION,\n",
    "        train_size=0.1,\n",
    "        save=True,\n",
    "        momentum=0.8\n",
    "    )\n",
    "\n",
    "    model = model_pipeline(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ede7d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros([2,3], dtype=torch.float32)\n",
    "a.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "908f85b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "         [-1.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "         [-1.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [ 1.,  1.,  0.,  ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
       "         [ 0.,  1.,  0.,  ...,  1.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  ..., -1., -1.,  0.],\n",
       "         [ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "         [-1.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [ 1.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  1.,  1.,  1.],\n",
       "         [ 0.,  1.,  0.,  ...,  1.,  1.,  1.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "         [ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "         [ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "         ...,\n",
       "         [ 0.,  0., -1.,  ...,  2.,  1.,  1.],\n",
       "         [-1., -1., -1.,  ...,  2.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  ...,  2.,  1.,  1.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data[1//5][0].to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32532af2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mb\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01faa290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,   4.,   9.,   7.,   8.,   4., 195., 151., 316.,  76.,   4., 157.,\n",
       "          3.,   5.,   2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data[4//5][1][4%5].to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b25e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f866a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a879f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283.12109375\n"
     ]
    }
   ],
   "source": [
    "import os, psutil; print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f6c010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Pau\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\wandb\\run-20230529_192133-tgmdo58a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grup10/pytorch-demo/runs/tgmdo58a' target=\"_blank\">glowing-fog-185</a></strong> to <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grup10/pytorch-demo/runs/tgmdo58a' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/tgmdo58a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glowing-fog-185</strong> at: <a href='https://wandb.ai/grup10/pytorch-demo/runs/tgmdo58a' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/tgmdo58a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230529_192133-tgmdo58a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"pytorch-demo\", config=config):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # execute only once to create the dataset\n",
    "        # generate_and_dump_dataset(config.root_dir, config.captions_file, config.transforms, cfg.DATA_LOCATION)\n",
    "\n",
    "        # Generate Dataset\n",
    "        dataset = make_dataset(config)\n",
    "\n",
    "        # make the data_loaders, and optimizer\n",
    "        train_loader, test_loader = make_dataloaders(config, dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e516d6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.296547889709473"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "my_iter = iter(train_loader)\n",
    "t1 = time.time()\n",
    "t0-t1\n",
    "# bs 32 nw all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae5e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d98d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   4,  28,   8,   4, 195, 151,  17,  32,  67,   4, 353,  11, 711,\n",
       "          8,  24,   3, 496,   5,   2], dtype=torch.int16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data[0//5][1][0%5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e378e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb7d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70173f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e5986ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.776714086532593"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "my_iter = iter(train_loader)\n",
    "t1 = time.time()\n",
    "t0-t1\n",
    "# bs 500 nw 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de2c3f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for a, b in my_iter:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2e5435a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "('{} cannot be pickled', '_MultiProcessingDataLoaderIter')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iter_2 \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\copy.py:161\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    159\u001b[0m reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43mreductor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:657\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;66;03m# TODO: add limited pickling support for sharing an iterator\u001b[39;00m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;66;03m# across multiple threads for HOGWILD.\u001b[39;00m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;66;03m# Probably the best way to do this is by moving the sample pushing\u001b[39;00m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# to a separate thread and then just sharing the data queue\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;66;03m# but signalling the end is tricky without a non-blocking API\u001b[39;00m\n\u001b[1;32m--> 657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be pickled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ('{} cannot be pickled', '_MultiProcessingDataLoaderIter')"
     ]
    }
   ],
   "source": [
    "iter_2 = deepcopy(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eef0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xnap-example",
   "language": "python",
   "name": "xnap-example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
