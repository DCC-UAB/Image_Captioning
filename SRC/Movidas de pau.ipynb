{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a69d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Pau\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\wandb\\run-20230530_223329-2yuqqulj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grup10/pytorch-demo/runs/2yuqqulj' target=\"_blank\">eternal-pyramid-259</a></strong> to <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grup10/pytorch-demo/runs/2yuqqulj' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/2yuqqulj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bc6c45d76542de80c0d26f65f45ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00050 examples: 8.011\n",
      "Loss after 00100 examples: 7.998\n",
      "Loss after 00150 examples: 7.958\n",
      "Loss after 00200 examples: 7.936\n",
      "Loss after 00250 examples: 7.909\n",
      "Loss after 00300 examples: 7.867\n",
      "Loss after 00350 examples: 7.837\n",
      "Loss after 00400 examples: 7.813\n",
      "Loss after 00450 examples: 7.767\n",
      "Loss after 00500 examples: 7.740\n",
      "Loss after 00550 examples: 7.709\n",
      "Loss after 00600 examples: 7.663\n",
      "Loss after 00650 examples: 7.578\n",
      "Loss after 00700 examples: 7.562\n",
      "Loss after 00750 examples: 7.524\n",
      "Loss after 00800 examples: 7.439\n",
      "Loss after 00850 examples: 7.347\n",
      "Loss after 00900 examples: 7.246\n",
      "Loss after 00950 examples: 7.216\n",
      "Loss after 01000 examples: 7.189\n",
      "Loss after 01050 examples: 7.115\n",
      "Loss after 01100 examples: 7.040\n",
      "Loss after 01150 examples: 7.001\n",
      "Loss after 01200 examples: 6.784\n",
      "Loss after 01250 examples: 6.747\n",
      "Loss after 01300 examples: 6.728\n",
      "Loss after 01350 examples: 6.566\n",
      "Loss after 01400 examples: 6.529\n",
      "Loss after 01450 examples: 6.367\n",
      "Loss after 01500 examples: 6.201\n",
      "Loss after 01550 examples: 6.274\n",
      "Loss after 01600 examples: 6.037\n",
      "Loss after 01650 examples: 6.091\n",
      "Loss after 01700 examples: 5.982\n",
      "Loss after 01750 examples: 5.990\n",
      "Loss after 01800 examples: 5.840\n",
      "Loss after 01850 examples: 5.730\n",
      "Loss after 01900 examples: 5.708\n",
      "Loss after 01950 examples: 5.750\n",
      "Loss after 02000 examples: 5.463\n",
      "Loss after 02050 examples: 5.516\n",
      "Loss after 02100 examples: 5.482\n",
      "Loss after 02150 examples: 5.430\n",
      "Loss after 02200 examples: 5.419\n",
      "Loss after 02250 examples: 5.407\n",
      "Loss after 02300 examples: 5.391\n",
      "Loss after 02350 examples: 5.361\n",
      "Loss after 02400 examples: 5.267\n",
      "Loss after 02450 examples: 5.332\n",
      "Loss after 02500 examples: 5.230\n",
      "Loss after 02550 examples: 5.220\n",
      "Loss after 02600 examples: 5.263\n",
      "Loss after 02650 examples: 5.196\n",
      "Loss after 02700 examples: 5.249\n",
      "Loss after 02750 examples: 5.139\n",
      "Loss after 02800 examples: 5.344\n",
      "Loss after 02850 examples: 5.217\n",
      "Loss after 02900 examples: 5.029\n",
      "Loss after 02950 examples: 4.984\n",
      "Loss after 03000 examples: 4.944\n",
      "Loss after 03050 examples: 5.101\n",
      "Loss after 03100 examples: 5.053\n",
      "Loss after 03150 examples: 5.024\n",
      "Loss after 03200 examples: 5.068\n",
      "Loss after 03250 examples: 4.983\n",
      "Loss after 03300 examples: 5.171\n",
      "Loss after 03350 examples: 4.927\n",
      "Loss after 03400 examples: 5.098\n",
      "Loss after 03450 examples: 4.977\n",
      "Loss after 03500 examples: 4.817\n",
      "Loss after 03550 examples: 5.032\n",
      "Loss after 03600 examples: 4.795\n",
      "Loss after 03650 examples: 5.078\n",
      "Loss after 03700 examples: 4.785\n",
      "Loss after 03750 examples: 4.941\n",
      "Loss after 03800 examples: 5.079\n",
      "Loss after 03850 examples: 4.896\n",
      "Loss after 03900 examples: 4.967\n",
      "Loss after 03950 examples: 5.061\n",
      "Loss after 04000 examples: 4.790\n",
      "Loss after 04050 examples: 4.747\n",
      "Loss after 04100 examples: 4.887\n",
      "Loss after 04150 examples: 4.882\n",
      "Loss after 04200 examples: 5.014\n",
      "Loss after 04250 examples: 4.839\n",
      "Loss after 04300 examples: 4.887\n",
      "Loss after 04350 examples: 4.872\n",
      "Loss after 04400 examples: 4.834\n",
      "Loss after 04450 examples: 4.840\n",
      "Loss after 04500 examples: 4.729\n",
      "Loss after 04550 examples: 4.863\n",
      "Loss after 04600 examples: 4.787\n",
      "Loss after 04650 examples: 4.812\n",
      "Loss after 04700 examples: 4.751\n",
      "Loss after 04750 examples: 4.817\n",
      "Loss after 04800 examples: 4.856\n",
      "Loss after 04850 examples: 4.877\n",
      "Loss after 04900 examples: 4.699\n",
      "Loss after 04950 examples: 4.690\n",
      "Loss after 05000 examples: 4.856\n",
      "Loss after 05050 examples: 4.935\n",
      "Loss after 05100 examples: 4.663\n",
      "Loss after 05150 examples: 4.713\n",
      "Loss after 05200 examples: 4.714\n",
      "Loss after 05250 examples: 4.777\n",
      "Loss after 05300 examples: 4.561\n",
      "Loss after 05350 examples: 4.842\n",
      "Loss after 05400 examples: 4.721\n",
      "Loss after 05450 examples: 4.662\n",
      "Loss after 05500 examples: 4.727\n",
      "Loss after 05550 examples: 4.794\n",
      "Loss after 05600 examples: 4.667\n",
      "Loss after 05650 examples: 4.777\n",
      "Loss after 05700 examples: 4.710\n",
      "Loss after 05750 examples: 4.678\n",
      "Loss after 05800 examples: 4.761\n",
      "Loss after 05850 examples: 4.554\n",
      "Loss after 05900 examples: 4.645\n",
      "Loss after 05950 examples: 4.709\n",
      "Loss after 06000 examples: 4.866\n",
      "Loss after 06050 examples: 4.695\n",
      "Loss after 06100 examples: 4.675\n",
      "Loss after 06150 examples: 4.620\n",
      "Loss after 06200 examples: 4.722\n",
      "Loss after 06250 examples: 4.617\n",
      "Loss after 06300 examples: 4.703\n",
      "Loss after 06350 examples: 4.906\n",
      "Loss after 06400 examples: 4.608\n",
      "Loss after 06450 examples: 4.574\n",
      "Loss after 06500 examples: 4.646\n",
      "Loss after 06550 examples: 4.749\n",
      "Loss after 06600 examples: 4.782\n",
      "Loss after 06650 examples: 4.569\n",
      "Loss after 06700 examples: 4.777\n",
      "Loss after 06750 examples: 4.647\n",
      "Loss after 06800 examples: 4.569\n",
      "Loss after 06850 examples: 4.648\n",
      "Loss after 06900 examples: 4.710\n",
      "Loss after 06950 examples: 4.585\n",
      "Loss after 07000 examples: 4.744\n",
      "Loss after 07050 examples: 4.448\n",
      "Loss after 07100 examples: 4.744\n",
      "Loss after 07150 examples: 4.644\n",
      "Loss after 07200 examples: 4.568\n",
      "Loss after 07250 examples: 4.602\n",
      "Loss after 07300 examples: 4.557\n",
      "Loss after 07350 examples: 4.520\n",
      "Loss after 07400 examples: 4.594\n",
      "Loss after 07450 examples: 4.598\n",
      "Loss after 07500 examples: 4.469\n",
      "Loss after 07550 examples: 4.622\n",
      "Loss after 07600 examples: 4.700\n",
      "Loss after 07650 examples: 4.523\n",
      "Loss after 07700 examples: 4.681\n",
      "Loss after 07750 examples: 4.539\n",
      "Loss after 07800 examples: 4.519\n",
      "Loss after 07850 examples: 4.506\n",
      "Loss after 07900 examples: 4.544\n",
      "Loss after 07950 examples: 4.381\n",
      "Loss after 08000 examples: 4.583\n",
      "Loss after 08050 examples: 4.527\n",
      "Loss after 08100 examples: 4.510\n",
      "Loss after 08150 examples: 4.471\n",
      "Loss after 08200 examples: 4.607\n",
      "Loss after 08250 examples: 4.534\n",
      "Loss after 08300 examples: 4.680\n",
      "Loss after 08350 examples: 4.622\n",
      "Loss after 08400 examples: 4.529\n",
      "Loss after 08450 examples: 4.691\n",
      "Loss after 08500 examples: 4.478\n",
      "Loss after 08550 examples: 4.587\n",
      "Loss after 08600 examples: 4.416\n",
      "Loss after 08650 examples: 4.639\n",
      "Loss after 08700 examples: 4.451\n",
      "Loss after 08750 examples: 4.405\n",
      "Loss after 08800 examples: 4.544\n",
      "Loss after 08850 examples: 4.474\n",
      "Loss after 08900 examples: 4.650\n",
      "Loss after 08950 examples: 4.439\n",
      "Loss after 09000 examples: 4.578\n",
      "Loss after 09050 examples: 4.309\n",
      "Loss after 09100 examples: 4.585\n",
      "Loss after 09150 examples: 4.555\n",
      "Loss after 09200 examples: 4.473\n",
      "Loss after 09250 examples: 4.488\n",
      "Loss after 09300 examples: 4.439\n",
      "Loss after 09350 examples: 4.664\n",
      "Loss after 09400 examples: 4.561\n",
      "Loss after 09450 examples: 4.489\n",
      "Loss after 09500 examples: 4.559\n",
      "Loss after 09550 examples: 4.495\n",
      "Loss after 09600 examples: 4.267\n",
      "Loss after 09650 examples: 4.472\n",
      "Loss after 09700 examples: 4.568\n",
      "Loss after 09750 examples: 4.392\n",
      "Loss after 09800 examples: 4.496\n",
      "Loss after 09850 examples: 4.493\n",
      "Loss after 09900 examples: 4.460\n",
      "Loss after 09950 examples: 4.489\n",
      "Loss after 10000 examples: 4.386\n",
      "Loss after 10050 examples: 4.506\n",
      "Loss after 10100 examples: 4.427\n",
      "Loss after 10150 examples: 4.354\n",
      "Loss after 10200 examples: 4.489\n",
      "Loss after 10250 examples: 4.384\n",
      "Loss after 10300 examples: 4.303\n",
      "Loss after 10350 examples: 4.565\n",
      "Loss after 10400 examples: 4.352\n",
      "Loss after 10450 examples: 4.558\n",
      "Loss after 10500 examples: 4.433\n",
      "Loss after 10550 examples: 4.363\n",
      "Loss after 10600 examples: 4.317\n",
      "Loss after 10650 examples: 4.387\n",
      "Loss after 10700 examples: 4.378\n",
      "Loss after 10750 examples: 4.480\n",
      "Loss after 10800 examples: 4.374\n",
      "Loss after 10850 examples: 4.598\n",
      "Loss after 10900 examples: 4.309\n",
      "Loss after 10950 examples: 4.487\n",
      "Loss after 11000 examples: 4.632\n",
      "Loss after 11050 examples: 4.429\n",
      "Loss after 11100 examples: 4.426\n",
      "Loss after 11150 examples: 4.561\n",
      "Loss after 11200 examples: 4.432\n",
      "Loss after 11250 examples: 4.327\n",
      "Loss after 11300 examples: 4.499\n",
      "Loss after 11350 examples: 4.304\n",
      "Loss after 11400 examples: 4.381\n",
      "Loss after 11450 examples: 4.292\n",
      "Loss after 11500 examples: 4.430\n",
      "Loss after 11550 examples: 4.439\n",
      "Loss after 11600 examples: 4.262\n",
      "Loss after 11650 examples: 4.317\n",
      "Loss after 11700 examples: 4.323\n",
      "Loss after 11750 examples: 4.498\n",
      "Loss after 11800 examples: 4.366\n",
      "Loss after 11850 examples: 4.538\n",
      "Loss after 11900 examples: 4.624\n",
      "Loss after 11950 examples: 4.297\n",
      "Loss after 12000 examples: 4.331\n",
      "Loss after 12050 examples: 4.323\n",
      "Loss after 12100 examples: 4.426\n",
      "Loss after 12150 examples: 4.380\n",
      "Loss after 12200 examples: 4.284\n",
      "Loss after 12250 examples: 4.285\n",
      "Loss after 12300 examples: 4.376\n",
      "Loss after 12350 examples: 4.221\n",
      "Loss after 12400 examples: 4.205\n",
      "Loss after 12450 examples: 4.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 12500 examples: 4.311\n",
      "Loss after 12550 examples: 4.257\n",
      "Loss after 12600 examples: 4.478\n",
      "Loss after 12650 examples: 4.279\n",
      "Loss after 12700 examples: 4.479\n",
      "Loss after 12750 examples: 4.261\n",
      "Loss after 12800 examples: 4.333\n",
      "Loss after 12850 examples: 4.343\n",
      "Loss after 12900 examples: 4.244\n",
      "Loss after 12950 examples: 4.389\n",
      "Loss after 13000 examples: 4.297\n",
      "Loss after 13050 examples: 4.347\n",
      "Loss after 13100 examples: 4.294\n",
      "Loss after 13150 examples: 4.437\n",
      "Loss after 13200 examples: 4.531\n",
      "Loss after 13250 examples: 4.291\n",
      "Loss after 13300 examples: 4.521\n",
      "Loss after 13350 examples: 4.337\n",
      "Loss after 13400 examples: 4.363\n",
      "Loss after 13450 examples: 4.234\n",
      "Loss after 13500 examples: 4.222\n",
      "Loss after 13550 examples: 4.428\n",
      "Loss after 13600 examples: 4.128\n",
      "Loss after 13650 examples: 4.162\n",
      "Loss after 13700 examples: 4.375\n",
      "Loss after 13750 examples: 4.297\n",
      "Loss after 13800 examples: 4.301\n",
      "Loss after 13850 examples: 4.233\n",
      "Loss after 13900 examples: 4.000\n",
      "Loss after 13950 examples: 4.263\n",
      "Loss after 14000 examples: 4.236\n",
      "Loss after 14050 examples: 4.308\n",
      "Loss after 14100 examples: 4.512\n",
      "Loss after 14150 examples: 4.223\n",
      "Loss after 14200 examples: 4.471\n",
      "Loss after 14250 examples: 4.186\n",
      "Loss after 14300 examples: 4.272\n",
      "Loss after 14350 examples: 4.204\n",
      "Loss after 14400 examples: 4.260\n",
      "Loss after 14450 examples: 4.390\n",
      "Loss after 14500 examples: 4.172\n",
      "Loss after 14550 examples: 4.284\n",
      "Loss after 14600 examples: 4.049\n",
      "Loss after 14650 examples: 4.365\n",
      "Loss after 14700 examples: 4.346\n",
      "Loss after 14750 examples: 4.268\n",
      "Loss after 14800 examples: 4.220\n",
      "Loss after 14850 examples: 4.157\n",
      "Loss after 14900 examples: 4.182\n",
      "Loss after 14950 examples: 4.350\n",
      "Loss after 15000 examples: 4.265\n",
      "Loss after 15050 examples: 4.280\n",
      "Loss after 15100 examples: 4.321\n",
      "Loss after 15150 examples: 4.396\n",
      "Loss after 15200 examples: 4.368\n",
      "Loss after 15250 examples: 4.164\n",
      "Loss after 15300 examples: 4.000\n",
      "Loss after 15350 examples: 4.351\n",
      "Loss after 15400 examples: 4.147\n",
      "Loss after 15450 examples: 4.192\n",
      "Loss after 15500 examples: 4.349\n",
      "Loss after 15550 examples: 4.005\n",
      "Loss after 15600 examples: 4.067\n",
      "Loss after 15650 examples: 4.390\n",
      "Loss after 15700 examples: 4.293\n",
      "Loss after 15750 examples: 4.442\n",
      "Loss after 15800 examples: 4.281\n",
      "Loss after 15850 examples: 4.170\n",
      "Loss after 15900 examples: 4.280\n",
      "Loss after 15950 examples: 4.010\n",
      "Loss after 16000 examples: 4.018\n",
      "Loss after 16050 examples: 4.201\n",
      "Loss after 16100 examples: 4.225\n",
      "Loss after 16150 examples: 4.139\n",
      "Loss after 16200 examples: 4.209\n",
      "Loss after 16250 examples: 4.151\n",
      "Loss after 16300 examples: 4.278\n",
      "Loss after 16350 examples: 4.226\n",
      "Loss after 16400 examples: 4.264\n",
      "Loss after 16450 examples: 4.241\n",
      "Loss after 16500 examples: 3.978\n",
      "Loss after 16550 examples: 3.985\n",
      "Loss after 16600 examples: 4.269\n",
      "Loss after 16650 examples: 4.057\n",
      "Loss after 16700 examples: 4.084\n",
      "Loss after 16750 examples: 4.204\n",
      "Loss after 16800 examples: 4.282\n",
      "Loss after 16850 examples: 4.262\n",
      "Loss after 16900 examples: 4.054\n",
      "Loss after 16950 examples: 4.065\n",
      "Loss after 17000 examples: 4.242\n",
      "Loss after 17050 examples: 4.177\n",
      "Loss after 17100 examples: 4.260\n",
      "Loss after 17150 examples: 4.091\n",
      "Loss after 17200 examples: 4.329\n",
      "Loss after 17250 examples: 3.884\n",
      "Loss after 17300 examples: 4.170\n",
      "Loss after 17350 examples: 4.249\n",
      "Loss after 17400 examples: 4.259\n",
      "Loss after 17450 examples: 4.273\n",
      "Loss after 17500 examples: 4.073\n",
      "Loss after 17550 examples: 3.917\n",
      "Loss after 17600 examples: 4.168\n",
      "Loss after 17650 examples: 4.206\n",
      "Loss after 17700 examples: 3.968\n",
      "Loss after 17750 examples: 4.036\n",
      "Loss after 17800 examples: 4.089\n",
      "Loss after 17850 examples: 4.196\n",
      "Loss after 17900 examples: 4.202\n",
      "Loss after 17950 examples: 4.184\n",
      "Loss after 18000 examples: 4.163\n",
      "Loss after 18050 examples: 4.150\n",
      "Loss after 18100 examples: 4.283\n",
      "Loss after 18150 examples: 4.298\n",
      "Loss after 18200 examples: 4.148\n",
      "Loss after 18250 examples: 4.043\n",
      "Loss after 18300 examples: 4.072\n",
      "Loss after 18350 examples: 4.212\n",
      "Loss after 18400 examples: 4.014\n",
      "Loss after 18450 examples: 4.062\n",
      "Loss after 18500 examples: 4.130\n",
      "Loss after 18550 examples: 4.097\n",
      "Loss after 18600 examples: 4.208\n",
      "Loss after 18650 examples: 4.030\n",
      "Loss after 18700 examples: 3.939\n",
      "Loss after 18750 examples: 4.104\n",
      "Loss after 18800 examples: 4.205\n",
      "Loss after 18850 examples: 4.172\n",
      "Loss after 18900 examples: 3.935\n",
      "Loss after 18950 examples: 4.101\n",
      "Loss after 19000 examples: 4.197\n",
      "Loss after 19050 examples: 3.950\n",
      "Loss after 19100 examples: 3.989\n",
      "Loss after 19150 examples: 4.078\n",
      "Loss after 19200 examples: 3.934\n",
      "Loss after 19250 examples: 4.123\n",
      "Loss after 19300 examples: 4.160\n",
      "Loss after 19350 examples: 4.051\n",
      "Loss after 19400 examples: 4.034\n",
      "Loss after 19450 examples: 4.016\n",
      "Loss after 19500 examples: 4.142\n",
      "Loss after 19550 examples: 4.056\n",
      "Loss after 19600 examples: 4.087\n",
      "Loss after 19650 examples: 4.223\n",
      "Loss after 19700 examples: 4.258\n",
      "Loss after 19750 examples: 3.842\n",
      "Loss after 19800 examples: 4.068\n",
      "Loss after 19850 examples: 4.187\n",
      "Loss after 19900 examples: 3.937\n",
      "Loss after 19950 examples: 4.098\n",
      "Loss after 20000 examples: 3.984\n",
      "Loss after 20050 examples: 4.220\n",
      "Loss after 20100 examples: 4.041\n",
      "Loss after 20150 examples: 4.111\n",
      "Loss after 20200 examples: 4.016\n",
      "Loss after 20250 examples: 4.066\n",
      "Loss after 20300 examples: 3.950\n",
      "Loss after 20350 examples: 3.912\n",
      "Loss after 20400 examples: 4.010\n",
      "Loss after 20450 examples: 4.310\n",
      "Loss after 20500 examples: 4.120\n",
      "Loss after 20550 examples: 4.077\n",
      "Loss after 20600 examples: 4.104\n",
      "Loss after 20650 examples: 4.122\n",
      "Loss after 20700 examples: 3.929\n",
      "Loss after 20750 examples: 4.134\n",
      "Loss after 20800 examples: 3.908\n",
      "Loss after 20850 examples: 4.171\n",
      "Loss after 20900 examples: 4.018\n",
      "Loss after 20950 examples: 4.134\n",
      "Loss after 21000 examples: 4.210\n",
      "Loss after 21050 examples: 3.933\n",
      "Loss after 21100 examples: 4.108\n",
      "Loss after 21150 examples: 3.971\n",
      "Loss after 21200 examples: 4.132\n",
      "Loss after 21250 examples: 4.045\n",
      "Loss after 21300 examples: 4.217\n",
      "Loss after 21350 examples: 4.058\n",
      "Loss after 21400 examples: 4.174\n",
      "Loss after 21450 examples: 4.227\n",
      "Loss after 21500 examples: 4.073\n",
      "Loss after 21550 examples: 4.057\n",
      "Loss after 21600 examples: 4.017\n",
      "Loss after 21650 examples: 4.044\n",
      "Loss after 21700 examples: 3.892\n",
      "Loss after 21750 examples: 4.145\n",
      "Loss after 21800 examples: 4.005\n",
      "Loss after 21850 examples: 3.939\n",
      "Loss after 21900 examples: 4.017\n",
      "Loss after 21950 examples: 3.868\n",
      "Loss after 22000 examples: 3.840\n",
      "Loss after 22050 examples: 4.160\n",
      "Loss after 22100 examples: 3.868\n",
      "Loss after 22150 examples: 3.899\n",
      "Loss after 22200 examples: 4.053\n",
      "Loss after 22250 examples: 4.003\n",
      "Loss after 22300 examples: 4.124\n",
      "Loss after 22350 examples: 4.020\n",
      "Loss after 22400 examples: 4.078\n",
      "Loss after 22450 examples: 3.904\n",
      "Loss after 22500 examples: 3.973\n",
      "Loss after 22550 examples: 3.942\n",
      "Loss after 22600 examples: 4.334\n",
      "Loss after 22650 examples: 3.814\n",
      "Loss after 22700 examples: 4.017\n",
      "Loss after 22750 examples: 3.916\n",
      "Loss after 22800 examples: 4.032\n",
      "Loss after 22850 examples: 3.761\n",
      "Loss after 22900 examples: 4.188\n",
      "Loss after 22950 examples: 3.889\n",
      "Loss after 23000 examples: 3.951\n",
      "Loss after 23050 examples: 3.861\n",
      "Loss after 23100 examples: 3.945\n",
      "Loss after 23150 examples: 4.027\n",
      "Loss after 23200 examples: 3.831\n",
      "Loss after 23250 examples: 4.182\n",
      "Loss after 23300 examples: 4.109\n",
      "Loss after 23350 examples: 4.131\n",
      "Loss after 23400 examples: 3.907\n",
      "Loss after 23450 examples: 4.026\n",
      "Loss after 23500 examples: 3.990\n",
      "Loss after 23550 examples: 3.729\n",
      "Loss after 23600 examples: 4.031\n",
      "Loss after 23650 examples: 3.890\n",
      "Loss after 23700 examples: 3.797\n",
      "Loss after 23750 examples: 3.792\n",
      "Loss after 23800 examples: 4.053\n",
      "Loss after 23850 examples: 3.872\n",
      "Loss after 23900 examples: 3.935\n",
      "Loss after 23950 examples: 4.023\n",
      "Loss after 24000 examples: 4.177\n",
      "Loss after 24050 examples: 3.871\n",
      "Loss after 24100 examples: 4.157\n",
      "Loss after 24150 examples: 3.901\n",
      "Loss after 24200 examples: 3.981\n",
      "Loss after 24250 examples: 3.790\n",
      "Loss after 24300 examples: 3.819\n",
      "Loss after 24350 examples: 3.947\n",
      "Loss after 24400 examples: 3.975\n",
      "Loss after 24450 examples: 4.151\n",
      "Loss after 24500 examples: 3.991\n",
      "Loss after 24550 examples: 4.130\n",
      "Loss after 24600 examples: 3.916\n",
      "Loss after 24650 examples: 3.745\n",
      "Loss after 24700 examples: 3.938\n",
      "Loss after 24750 examples: 4.123\n",
      "Loss after 24800 examples: 4.008\n",
      "Loss after 24850 examples: 3.930\n",
      "Loss after 24900 examples: 4.216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 24950 examples: 3.933\n",
      "Loss after 25000 examples: 3.885\n",
      "Loss after 25050 examples: 3.983\n",
      "Loss after 25100 examples: 3.848\n",
      "Loss after 25150 examples: 3.960\n",
      "Loss after 25200 examples: 4.175\n",
      "Loss after 25250 examples: 3.940\n",
      "Loss after 25300 examples: 4.027\n",
      "Loss after 25350 examples: 3.903\n",
      "Loss after 25400 examples: 3.887\n",
      "Loss after 25450 examples: 3.857\n",
      "Loss after 25500 examples: 3.872\n",
      "Loss after 25550 examples: 4.003\n",
      "Loss after 25600 examples: 4.032\n",
      "Loss after 25650 examples: 3.909\n",
      "Loss after 25700 examples: 4.090\n",
      "Loss after 25750 examples: 3.887\n",
      "Loss after 25800 examples: 4.043\n",
      "Loss after 25850 examples: 3.975\n",
      "Loss after 25900 examples: 4.066\n",
      "Loss after 25950 examples: 4.074\n",
      "Loss after 26000 examples: 3.989\n",
      "Loss after 26050 examples: 4.067\n",
      "Loss after 26100 examples: 4.022\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00e92c83d814c5b8a68bb093185538b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▅▄▄▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▁▁▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>4.02217</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eternal-pyramid-259</strong> at: <a href='https://wandb.ai/grup10/pytorch-demo/runs/2yuqqulj' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/2yuqqulj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230530_223329-2yuqqulj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 141\u001b[0m\n\u001b[0;32m    114\u001b[0m DATA_LOCATION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    116\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39mDATA_LOCATION\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Images\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     decoder_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m    139\u001b[0m )\n\u001b[1;32m--> 141\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 77\u001b[0m, in \u001b[0;36mmodel_pipeline\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     my_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 77\u001b[0m     train_loss_arr_aux, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     my_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Testing\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\train.py:20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, criterion, optimizer, config, epoch, verbatim)\u001b[0m\n\u001b[0;32m     16\u001b[0m loss_arr_batch \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Losses of the batches\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (image, captions) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_loader)):\n\u001b[1;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     example_ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(image)\n\u001b[0;32m     22\u001b[0m     batch_ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\train.py:41\u001b[0m, in \u001b[0;36mtrain_batch\u001b[1;34m(image, captions, model, vocab_size, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Feed forward\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m outputs, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Calculate the batch loss.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m targets \u001b[38;5;241m=\u001b[39m captions[:, \u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32m~\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\models\\models.py:181\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[1;34m(self, images, captions)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, captions):\n\u001b[1;32m--> 181\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(features, captions)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\models\\models.py:30\u001b[0m, in \u001b[0;36mEncoderCNN.forward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[1;32m---> 30\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size,2048,7,7)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size,7,7,2048)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mview(features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# (batch_size,49,2048)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from train import *\n",
    "from test import *\n",
    "from utils.utils import *\n",
    "from models.models import *\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "global device\n",
    "\n",
    "import os\n",
    "\n",
    "# Setting CUDA ALLOC split size to 256 to avoid running out of memory\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "# Stopping wandb from creating symlinks\n",
    "os.environ[\"WANDB_DISABLE_SYMLINKS\"] = \"true\"\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def model_pipeline(cfg: dict):\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"pytorch-demo\", config=cfg):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # Execute only once to create the dataset\n",
    "        # generate_and_dump_dataset(config.root_dir, config.captions_file, config.transforms, cfg.DATA_LOCATION)\n",
    "\n",
    "        # Generate Dataset\n",
    "        dataset = make_dataset(config)\n",
    "\n",
    "        # Get the data loaders\n",
    "        train_loader, test_loader = make_dataloaders(config, dataset, 1)\n",
    "\n",
    "        # Generate vocab\n",
    "        vocab = dataset.vocab\n",
    "        config.vocab_size = len(vocab)\n",
    "\n",
    "        # Get the model\n",
    "        my_model = make_model(config, device)\n",
    "\n",
    "        # Define the loss and optimizer\n",
    "        criterion = get_criterion(config.criterion, vocab.stoi[\"<PAD>\"])\n",
    "        criterion.ignore_index=vocab.stoi[\"<PAD>\"]\n",
    "        \n",
    "        optimizer = get_optimizer(config.optimizer, my_model.parameters(), config.learning_rate)\n",
    "        \n",
    "        # Arrays to log data\n",
    "        train_loss_arr_epoch, test_loss_arr_epoch, acc_arr_epoch  = [], [], [] # Epoch-wise\n",
    "        train_loss_arr_batch, test_loss_arr_batch, acc_arr_batch = [], [], [] # Batch-wise\n",
    "        train_execution_times, test_execution_times = [], [] # Execution times\n",
    "\n",
    "        \n",
    "        for epoch in tqdm(range(1, config.epochs + 1)):\n",
    "            # Training\n",
    "            my_model.train()\n",
    "            train_loss_arr_aux, train_time = train(my_model, train_loader, criterion, optimizer, config, epoch)\n",
    "            my_model.eval()\n",
    "\n",
    "            # Testing\n",
    "            acc_arr_aux, test_loss_arr_aux, test_time = test(my_model, test_loader, criterion, vocab, config, device)\n",
    "\n",
    "            # Check how model performs\n",
    "            test_model_performance(my_model, test_loader, device, vocab, epoch, config)\n",
    "            \n",
    "            # Logging data for vizz\n",
    "            train_loss_arr_epoch.append(np.mean(train_loss_arr_aux)); test_loss_arr_epoch.append(np.mean(test_loss_arr_aux))\n",
    "            train_loss_arr_batch += train_loss_arr_aux; test_loss_arr_batch += test_loss_arr_aux\n",
    "            acc_arr_epoch.append(np.mean(acc_arr_aux)); acc_arr_batch += acc_arr_aux\n",
    "            train_execution_times.append(train_time); test_execution_times.append(test_time)\n",
    "\n",
    "            \n",
    "        if config.save:\n",
    "            export_data(train_loss_arr_epoch, test_loss_arr_epoch, acc_arr_epoch, train_execution_times, test_execution_times,\n",
    "                   train_loss_arr_batch, acc_arr_batch, test_loss_arr_batch, config)\n",
    "            \n",
    "            save_model(my_model, config, config.DATA_LOCATION+'/logs'+'/EncoderDecorder_model.pth')\n",
    "\n",
    "    return my_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "\n",
    "    print(\"Using: \", device)\n",
    "\n",
    "    transforms = T.Compose([\n",
    "        T.Resize(226),\n",
    "        T.RandomCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    DATA_LOCATION = '../data'\n",
    "\n",
    "    config = dict(\n",
    "        # Paths\n",
    "        root_dir=DATA_LOCATION+\"/Images\",\n",
    "        captions_file=DATA_LOCATION+\"/captions.txt\",\n",
    "        DATA_LOCATION=DATA_LOCATION,\n",
    "        save=True,\n",
    "\n",
    "        # Training data\n",
    "        epochs=1,\n",
    "        batch_size=50,\n",
    "        train_size=0.8,\n",
    "        \n",
    "        # Model data\n",
    "        optimizer='Adam',\n",
    "        criterion='CrossEntropy',\n",
    "        learning_rate=0.0001,\n",
    "        device=device,\n",
    "        encoder='VGG',\n",
    "        transforms=transforms,\n",
    "        embed_size=300,\n",
    "        attention_dim=256,\n",
    "        encoder_dim= 512,  # GoogleNet 1024, RESNET/densenet: 2048\n",
    "        decoder_dim=512,\n",
    "    )\n",
    "\n",
    "    model = model_pipeline(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62754c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3333333333333335"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88d23c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4222e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Pau\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\wandb\\run-20230530_154748-w7db1g0b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grup10/pytorch-demo/runs/w7db1g0b' target=\"_blank\">glamorous-frost-235</a></strong> to <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grup10/pytorch-demo/runs/w7db1g0b' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/w7db1g0b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d8c28aaaad4942bbdbb2805a276339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.017 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.066979…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glamorous-frost-235</strong> at: <a href='https://wandb.ai/grup10/pytorch-demo/runs/w7db1g0b' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/w7db1g0b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230530_154748-w7db1g0b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'ignore_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Define the loss and optimizer\u001b[39;00m\n\u001b[0;32m     22\u001b[0m criterion \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mcriterion\n\u001b[1;32m---> 23\u001b[0m criterion\u001b[38;5;241m.\u001b[39mignore_index\u001b[38;5;241m=\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstoi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mparms \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mparameters()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'ignore_index'"
     ]
    }
   ],
   "source": [
    "with wandb.init(project=\"pytorch-demo\", config=config):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # Execute only once to create the dataset\n",
    "        # generate_and_dump_dataset(config.root_dir, config.captions_file, config.transforms, cfg.DATA_LOCATION)\n",
    "\n",
    "        # Generate Dataset\n",
    "        dataset = make_dataset(config)\n",
    "\n",
    "        # Get the data loaders\n",
    "        train_loader, test_loader = make_dataloaders(config, dataset, 1)\n",
    "\n",
    "        # Generate vocab\n",
    "        vocab = dataset.vocab\n",
    "        config.vocab_size = len(vocab)\n",
    "\n",
    "        # Get the model\n",
    "        my_model = make_model(config, device)\n",
    "\n",
    "        # Define the loss and optimizer\n",
    "        criterion = config.criterion\n",
    "        criterion.ignore_index=vocab.stoi[\"<PAD>\"]\n",
    "        \n",
    "        optimizer = config.optimizer\n",
    "        optimizer.parms = my_model.parameters()\n",
    "        optimizer.lr = config.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589dbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac854db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681eee3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e2142fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit.ignore_index = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6757087",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a0fdc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(params=[torch.zeros([4,2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19475b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adam.Adam"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f6c010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Pau\\Desktop\\PycharmProjects\\xnap-project-matcad_grup_10\\SRC\\wandb\\run-20230530_131140-t9rs5a0n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n' target=\"_blank\">deep-violet-228</a></strong> to <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grup10/pytorch-demo' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-violet-228</strong> at: <a href='https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n' target=\"_blank\">https://wandb.ai/grup10/pytorch-demo/runs/t9rs5a0n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230530_131140-t9rs5a0n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"pytorch-demo\", config=config):\n",
    "    config = wandb.config\n",
    "\n",
    "    # Generate Dataset\n",
    "    dataset = make_dataset(config)\n",
    "\n",
    "    # make the data_loaders, and optimizer\n",
    "    #train_loader, test_loader = make_dataloaders(config, dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6cdd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72e5bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[[-0.9019, -0.1315, -0.2341,  ..., -1.3301, -1.5527, -1.2783],\n",
       "           [-1.0215, -0.1829, -0.2170,  ..., -1.4502, -1.5869, -1.6895],\n",
       "           [-1.0566, -0.1656, -0.2000,  ..., -1.6211, -1.5527, -1.6387],\n",
       "           ...,\n",
       "           [ 1.6670,  1.4951,  0.4680,  ...,  1.5293,  0.6733,  0.7246],\n",
       "           [ 0.8789,  0.1083,  0.0056,  ...,  1.4951,  0.6733,  0.6904],\n",
       "           [ 0.6904,  1.5469,  0.9644,  ...,  1.4951,  0.7075,  0.6904]],\n",
       "  \n",
       "          [[-0.7925,  0.0301, -0.0049,  ..., -1.2832, -1.5107, -0.9502],\n",
       "           [-0.9678, -0.0224,  0.0476,  ..., -1.4229, -1.5635, -1.5801],\n",
       "           [-1.0029, -0.0049,  0.0301,  ..., -1.5459, -1.4404, -1.5459],\n",
       "           ...,\n",
       "           [ 1.2207,  0.7656, -0.6177,  ...,  1.8506,  1.2031,  1.1855],\n",
       "           [-0.2500, -0.4775, -0.4602,  ...,  1.8330,  1.2207,  1.2031],\n",
       "           [ 0.0476,  1.0459,  0.3103,  ...,  1.8154,  1.2031,  1.1855]],\n",
       "  \n",
       "          [[-0.6021,  0.1302,  0.0779,  ..., -1.2812, -1.4902, -1.2637],\n",
       "           [-0.6890,  0.0779,  0.0605,  ..., -1.4033, -1.4902, -1.5781],\n",
       "           [-0.7935,  0.0953,  0.1302,  ..., -1.5254, -1.4033, -1.5781],\n",
       "           ...,\n",
       "           [-0.2358, -0.4275, -1.2295,  ...,  2.3086,  1.8906,  1.7861],\n",
       "           [-1.0547, -1.0898, -1.0898,  ...,  2.2910,  1.8906,  1.7686],\n",
       "           [-0.6714, -0.2358, -0.7759,  ...,  2.2734,  1.8555,  1.7510]]],\n",
       "         dtype=torch.float16),\n",
       "  [tensor([  1,   4,  28,   8,   4, 195, 151,  17,  32,  67,   4, 353,  11, 711,\n",
       "             8,  24,   3, 496,   5,   2]),\n",
       "   tensor([  1,   4,   7, 316,  76,   4, 157,  74,   5,   2]),\n",
       "   tensor([   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2]),\n",
       "   tensor([   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2]),\n",
       "   tensor([  1,   4,   9,   7,   8,   4, 195, 151, 316,  76,   4, 157,   3,   5,\n",
       "             2])]],\n",
       " [tensor([[[-1.4160, -1.4160, -1.4160,  ..., -1.1758, -1.2959, -1.2959],\n",
       "           [-1.4326, -1.4326, -1.4160,  ..., -1.3301, -1.3135, -1.2441],\n",
       "           [-1.4502, -1.4326, -1.4502,  ..., -1.5527, -1.4844, -1.2275],\n",
       "           ...,\n",
       "           [ 0.1940,  0.1255,  0.3652,  ..., -0.4397, -0.4739, -0.2341],\n",
       "           [ 0.1426,  0.2281,  0.1768,  ..., -0.1486, -0.1486, -0.2000],\n",
       "           [ 0.2111,  0.0398, -0.1656,  ..., -0.1143, -0.3540, -0.4910]],\n",
       "  \n",
       "          [[-1.3525, -1.3525, -1.3525,  ..., -0.6177, -0.7925, -0.8804],\n",
       "           [-1.3701, -1.3701, -1.3525,  ..., -0.8804, -0.8804, -0.8452],\n",
       "           [-1.3877, -1.3701, -1.3877,  ..., -1.2129, -1.1426, -0.8804],\n",
       "           ...,\n",
       "           [ 0.2927,  0.2227,  0.4502,  ..., -0.3550, -0.3901, -0.1449],\n",
       "           [ 0.2052,  0.3276,  0.2578,  ..., -0.0399, -0.0399, -0.0924],\n",
       "           [ 0.3103,  0.1876, -0.0399,  ..., -0.0049, -0.2676, -0.3726]],\n",
       "  \n",
       "          [[-1.1074, -1.1074, -1.1074,  ..., -1.1592, -1.2812, -1.3164],\n",
       "           [-1.1250, -1.1250, -1.1074,  ..., -1.2119, -1.1943, -1.2471],\n",
       "           [-1.1426, -1.1250, -1.1426,  ..., -1.2988, -1.2295, -1.1943],\n",
       "           ...,\n",
       "           [ 0.5835,  0.5137,  0.7056,  ...,  0.0779,  0.0082,  0.2522],\n",
       "           [ 0.5312,  0.6528,  0.5483,  ...,  0.2871,  0.3044,  0.2871],\n",
       "           [ 0.6006,  0.4961,  0.2695,  ...,  0.3394,  0.1128,  0.0256]]],\n",
       "         dtype=torch.float16),\n",
       "  [tensor([  1,   4,  20,   6,  16,   4, 680,   6,  34, 694,   2]),\n",
       "   tensor([  1,   4,  20,   6,  16,   4, 898, 116, 239,   6,  41,  12, 106, 107,\n",
       "            13,  10, 177,   5,   2]),\n",
       "   tensor([  1,   4,  20,   6,  16,   4,  21,   6,  12,  30, 930,  34, 837,  19,\n",
       "           106, 107,   8,  10,  65,   5,   2]),\n",
       "   tensor([   1,   50,   51,   11,  321, 1904,   78,   19,  106,  107,   13,   10,\n",
       "            177,    5,    2]),\n",
       "   tensor([  1,  50,  51,  13, 592, 622, 544, 106, 107,   5,   2])]],\n",
       " [tensor([[[ 1.4951,  1.5127,  1.5127,  ...,  1.7012,  1.7178,  1.7012],\n",
       "           [ 1.4443,  1.4609,  1.4609,  ...,  1.7178,  1.7178,  1.7012],\n",
       "           [ 1.3926,  1.4268,  1.4443,  ...,  1.7178,  1.7012,  1.6836],\n",
       "           ...,\n",
       "           [-0.5596, -1.0908, -1.0732,  ..., -0.5938, -0.5254, -0.3198],\n",
       "           [-0.2000, -0.9707, -1.0215,  ..., -0.4568, -0.6792, -0.2341],\n",
       "           [-0.5767, -0.5767, -0.9878,  ..., -0.4397, -0.8335, -0.1829]],\n",
       "  \n",
       "          [[ 1.7812,  1.7979,  1.7979,  ...,  1.9736,  1.9912,  1.9912],\n",
       "           [ 1.7285,  1.7461,  1.7461,  ...,  1.9912,  1.9912,  2.0254],\n",
       "           [ 1.7109,  1.7109,  1.7285,  ...,  1.9912,  1.9912,  2.0078],\n",
       "           ...,\n",
       "           [ 0.0476, -0.5479, -0.4951,  ..., -0.0224, -0.0049,  0.2052],\n",
       "           [ 0.2227, -0.4951, -0.3376,  ...,  0.1702, -0.0750,  0.2751],\n",
       "           [-0.1274, -0.0750, -0.3726,  ...,  0.0476, -0.1799,  0.3452]],\n",
       "  \n",
       "          [[ 1.8730,  1.8906,  1.8906,  ...,  2.1523,  2.1699,  2.1699],\n",
       "           [ 1.8213,  1.8379,  1.8379,  ...,  2.1699,  2.1523,  2.1523],\n",
       "           [ 1.7686,  1.7861,  1.8037,  ...,  2.1699,  2.1699,  2.1699],\n",
       "           ...,\n",
       "           [-0.7588, -1.2637, -1.0723,  ..., -1.1426, -1.0205, -1.0029],\n",
       "           [-0.5845, -1.1592, -1.1943,  ..., -0.7588, -1.0723, -0.6890],\n",
       "           [-0.8633, -0.9331, -1.1426,  ..., -0.9678, -1.3691, -0.5146]]],\n",
       "         dtype=torch.float16),\n",
       "  [tensor([   1,    4,    9,    7,  114,    8, 1447,   77,    8,   23,   11,    4,\n",
       "            325,  643,   12,  104,  229,    8,    4, 1301,    5,    2]),\n",
       "   tensor([  1,   4,   9,   7,  17,  46,   8,  23,  11,   4,  53, 325, 643,   5,\n",
       "             2]),\n",
       "   tensor([   1,    4,   85,    7,    8,   10,   22,   92,   12,    3,    8,   23,\n",
       "             11,    4,   21, 2306,   12,    4,  643,   13,   64,    5,    2]),\n",
       "   tensor([  1, 204,  17,   4,   7,  12,  91,  46,   8,  23,  11,   4, 643, 176,\n",
       "             5,   2]),\n",
       "   tensor([  1,  61,   7,  12,  91, 176,  70,   8,  10,  22,   5,   2])]]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11483cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0fd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9cc584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db2742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iter = iter(train_loader)\n",
    "img, cap = next(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffbbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "681c34d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016640c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4702a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2, cap2 = next(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c182b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67734e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ebcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f300718",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,captions = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99653d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, captions = image.to(device), captions.to(device)\n",
    "\n",
    "# Zero the gradients.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Feed forward\n",
    "outputs, attentions = my_model(image.to(torch.float32), captions)\n",
    "\n",
    "# Calculate the batch loss.\n",
    "targets = captions[:, 1:]\n",
    "loss = criterion(outputs.view(-1, config.vocab_size), targets.reshape(-1))\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Update the parameters in the optimizer.\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69795396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0419, -0.0200, -0.0398,  ..., -0.0310,  0.0729,  0.0062],\n",
       "        [ 0.1156, -0.0232, -0.1058,  ...,  0.0588,  0.0166, -0.1509],\n",
       "        [ 0.0336,  0.0681, -0.1028,  ...,  0.0521,  0.0040, -0.1283],\n",
       "        ...,\n",
       "        [-0.2936,  0.1309,  0.0571,  ..., -0.1252,  0.2291, -0.0856],\n",
       "        [-0.2734,  0.2516, -0.0050,  ...,  0.0603, -0.0179, -0.0015],\n",
       "        [-0.2988,  0.1344, -0.0523,  ...,  0.0208, -0.0471,  0.0206]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.view(-1, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac22fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 20])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41efcc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0419, -0.0200, -0.0398,  ..., -0.0310,  0.0729,  0.0062],\n",
       "        [ 0.1156, -0.0232, -0.1058,  ...,  0.0588,  0.0166, -0.1509],\n",
       "        [ 0.0336,  0.0681, -0.1028,  ...,  0.0521,  0.0040, -0.1283],\n",
       "        ...,\n",
       "        [-0.2936,  0.1309,  0.0571,  ..., -0.1252,  0.2291, -0.0856],\n",
       "        [-0.2734,  0.2516, -0.0050,  ...,  0.0603, -0.0179, -0.0015],\n",
       "        [-0.2988,  0.1344, -0.0523,  ...,  0.0208, -0.0471,  0.0206]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.view(-1, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training and track with wandb\n",
    "example_ct = 0  # number of examples seen\n",
    "batch_ct = 0\n",
    "\n",
    "loss_arr_batch = []  # Losses of the batches\n",
    "\n",
    "for idx, (image, captions) in enumerate(iter(data_loader)):\n",
    "\n",
    "    loss = train_batch(image.to(torch.float32), captions, model, config.vocab_size, optimizer, criterion, device=config.device)\n",
    "    example_ct += len(image)\n",
    "    batch_ct += 1\n",
    "\n",
    "    loss_arr_batch.append(loss.tolist())\n",
    "\n",
    "    # Report metrics every 1th batch\n",
    "    if ((batch_ct + 1) % 1) == 0 and verbatim:\n",
    "        train_log(loss, example_ct, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba8be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41984f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b5b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28def3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71b03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c40f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc53be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad055988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e516d6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.296547889709473"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "my_iter = iter(train_loader)\n",
    "t1 = time.time()\n",
    "t0-t1\n",
    "# bs 32 nw all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae5e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d98d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   4,  28,   8,   4, 195, 151,  17,  32,  67,   4, 353,  11, 711,\n",
       "          8,  24,   3, 496,   5,   2], dtype=torch.int16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data[0//5][1][0%5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e378e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,   28,    8,    4,  195,  151,   17,   32,   67,    4,  353,\n",
       "           11,  711,    8,   24,    3,  496,    5,    2],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,    8,    4,  195,  151,  316,   76,    4,  157,\n",
       "            3,    5,    2,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   76,    4,  157, 2409,    5,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    9,    7,   32,   10,  711,   27,  104, 2409,    5,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    4,    7,  316,   76,    4,  157,   74,    5,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb7d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70173f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e5986ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.776714086532593"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "my_iter = iter(train_loader)\n",
    "t1 = time.time()\n",
    "t0-t1\n",
    "# bs 500 nw 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de2c3f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for a, b in my_iter:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2e5435a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "('{} cannot be pickled', '_MultiProcessingDataLoaderIter')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iter_2 \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\copy.py:161\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    159\u001b[0m reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce_ex__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43mreductor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__reduce__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\xnap-example\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:657\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;66;03m# TODO: add limited pickling support for sharing an iterator\u001b[39;00m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;66;03m# across multiple threads for HOGWILD.\u001b[39;00m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;66;03m# Probably the best way to do this is by moving the sample pushing\u001b[39;00m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# to a separate thread and then just sharing the data queue\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;66;03m# but signalling the end is tricky without a non-blocking API\u001b[39;00m\n\u001b[1;32m--> 657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be pickled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: ('{} cannot be pickled', '_MultiProcessingDataLoaderIter')"
     ]
    }
   ],
   "source": [
    "iter_2 = deepcopy(my_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eef0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xnap-example",
   "language": "python",
   "name": "xnap-example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
